{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1696d75-d304-46ea-bbbc-9108207d4b3f",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce26c37a-c3bc-4fb6-a473-e446d0c42833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from shapely.geometry import Point\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.base import clone\n",
    "from joblib import dump\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64bfea-9b4c-4e19-a17f-094c302b4bc0",
   "metadata": {},
   "source": [
    "### Feature Toggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb07d95-c997-48ba-9bc1-6ed05fd3f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_FLAGS = {\n",
    "    \"building_cov_50m\": True,\n",
    "    \"building_cov_100m\": True,\n",
    "    \"building_cov_200m\": True,\n",
    "    \"distance_water\": True,\n",
    "    \"distance_parks\": True,\n",
    "    \"lst_value\": True,      # We assume you have a valid LST raster\n",
    "    \"ndvi_value\": True,     # We assume your S2 Indices raster has NDVI in band 1\n",
    "    \"ndbi_value\": True,     # band 2\n",
    "    \"ndwi_value\": True,     # band 3\n",
    "    \"evi_value\": False,     # only if you have B02,B04,B08 raw reflectances\n",
    "    \"dist_manhattan_centre\": True,\n",
    "    \"dist_bronx_centre\": True,\n",
    "    \"location_cluster\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2dfdb6-4bc0-4695-853a-1690e223feea",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4daab130-64be-433b-9b10-57f1d3734a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def building_coverage_fraction(geom, building_gdf, radius=50):\n",
    "    buffer_poly = geom.buffer(radius)\n",
    "    clipped = gpd.clip(building_gdf, buffer_poly)\n",
    "    area_buildings = clipped.geometry.area.sum()\n",
    "    area_buf = buffer_poly.area\n",
    "    return area_buildings / area_buf if area_buf > 0 else 0\n",
    "\n",
    "def distance_to_polygons(geom, poly_gdf):\n",
    "    dists = poly_gdf.geometry.distance(geom)\n",
    "    return dists.min() if len(dists) > 0 else np.nan\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "def extract_raster_value(geom, raster, band_index=1, method=\"nearest\"):\n",
    "    x, y = geom.x, geom.y\n",
    "    val = raster.sel(x=x, y=y, band=band_index, method=method).values\n",
    "    return float(val)\n",
    "\n",
    "def compute_evi(blue, red, nir, L=1.0, C1=6.0, C2=7.5, G=2.5):\n",
    "    return G * (nir - red) / (nir + C1 * red - C2 * blue + L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2cdb4-d674-4380-89e7-47f59c90c474",
   "metadata": {},
   "source": [
    "### LOAD BOROUGH BOUNDARIES, BUILDINGS, WATER, PARKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76b6bfd-7d16-4de4-af39-7ba6ae4768e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading boroughs + buildings...\n",
      "     Name Description                                           geometry  \\\n",
      "5926                   MULTIPOLYGON (((1003404.964 249576.503, 100335...   \n",
      "5927                   MULTIPOLYGON (((1000278.304 241985.812, 100017...   \n",
      "5928                   MULTIPOLYGON (((998336.549 242281.176, 998273....   \n",
      "5929                   MULTIPOLYGON (((999175.11 243083.583, 998935.9...   \n",
      "5930                   MULTIPOLYGON (((1000201.166 243560.061, 100022...   \n",
      "...   ...         ...                                                ...   \n",
      "9431                   MULTIPOLYGON (((997358.658 223175.61, 997394.0...   \n",
      "9432                   MULTIPOLYGON (((998198.469 222047.008, 998288....   \n",
      "9433                   MULTIPOLYGON (((997517.49 219375.698, 997618.2...   \n",
      "9434                   MULTIPOLYGON (((997465.413 215819.05, 997419.4...   \n",
      "9435                   MULTIPOLYGON (((996496.699 217310.85, 996432.4...   \n",
      "\n",
      "      index_right   BoroName  cartodb_id                       created_at  \\\n",
      "5926          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "5927          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "5928          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "5929          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "5930          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "...           ...        ...         ...                              ...   \n",
      "9431          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "9432          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "9433          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "9434          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "9435          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
      "\n",
      "                           updated_at  \n",
      "5926 2013-03-09 02:42:03.989000+00:00  \n",
      "5927 2013-03-09 02:42:03.989000+00:00  \n",
      "5928 2013-03-09 02:42:03.989000+00:00  \n",
      "5929 2013-03-09 02:42:03.989000+00:00  \n",
      "5930 2013-03-09 02:42:03.989000+00:00  \n",
      "...                               ...  \n",
      "9431 2013-03-09 02:42:03.989000+00:00  \n",
      "9432 2013-03-09 02:42:03.989000+00:00  \n",
      "9433 2013-03-09 02:42:03.989000+00:00  \n",
      "9434 2013-03-09 02:42:03.989000+00:00  \n",
      "9435 2013-03-09 02:42:03.989000+00:00  \n",
      "\n",
      "[3495 rows x 8 columns]\n",
      "     Name Description                                           geometry  \\\n",
      "0                      MULTIPOLYGON (((1006651.793 248309.569, 100656...   \n",
      "1                      MULTIPOLYGON (((1005842.639 248829.838, 100585...   \n",
      "2                      MULTIPOLYGON (((1006243.634 249006.538, 100625...   \n",
      "3                      MULTIPOLYGON (((1006227.161 249476.52, 1006235...   \n",
      "4                      MULTIPOLYGON (((1008500.118 249763.236, 100846...   \n",
      "...   ...         ...                                                ...   \n",
      "5924                   MULTIPOLYGON (((1013724.683 234459.218, 101372...   \n",
      "5925                   MULTIPOLYGON (((1018783.438 232061.278, 101875...   \n",
      "5994                   MULTIPOLYGON (((1002689.38 242521.279, 1002660...   \n",
      "6335                   MULTIPOLYGON (((1005495.119 249503.556, 100544...   \n",
      "7919                   MULTIPOLYGON (((1005676.9 249791.544, 1005651....   \n",
      "\n",
      "      index_right BoroName  cartodb_id                       created_at  \\\n",
      "0             4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "1             4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "2             4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "3             4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "4             4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "...           ...      ...         ...                              ...   \n",
      "5924          4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "5925          4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "5994          4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "6335          4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "7919          4.0    Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
      "\n",
      "                           updated_at  \n",
      "0    2013-03-09 02:42:03.989000+00:00  \n",
      "1    2013-03-09 02:42:03.989000+00:00  \n",
      "2    2013-03-09 02:42:03.989000+00:00  \n",
      "3    2013-03-09 02:42:03.989000+00:00  \n",
      "4    2013-03-09 02:42:03.989000+00:00  \n",
      "...                               ...  \n",
      "5924 2013-03-09 02:42:03.989000+00:00  \n",
      "5925 2013-03-09 02:42:03.989000+00:00  \n",
      "5994 2013-03-09 02:42:03.989000+00:00  \n",
      "6335 2013-03-09 02:42:03.989000+00:00  \n",
      "7919 2013-03-09 02:42:03.989000+00:00  \n",
      "\n",
      "[5927 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading boroughs + buildings...\")\n",
    "gdf_boroughs = gpd.read_file(\"./data/nyc_boroughs.geojson\").to_crs(\"EPSG:2263\")\n",
    "if \"name\" in gdf_boroughs.columns:\n",
    "    gdf_boroughs.rename(columns={\"name\":\"BoroName\"}, inplace=True)\n",
    "\n",
    "gdf_buildings = gpd.read_file(\"./data/Building_Footprint.kml\").to_crs(\"EPSG:2263\")\n",
    "gdf_buildings = gpd.sjoin(gdf_buildings, gdf_boroughs, how=\"left\", predicate=\"intersects\")\n",
    "gdf_buildings = gdf_buildings.dropna(subset=[\"BoroName\"])\n",
    "\n",
    "gdf_water = pd.read_csv(\"./data/NYC_Planimetric_Database__Hydrography_20250123.csv\")\n",
    "from shapely import wkt\n",
    "gdf_water[\"geometry\"] = gdf_water[\"the_geom\"].apply(wkt.loads)\n",
    "gdf_water = gpd.GeoDataFrame(gdf_water, geometry=\"geometry\", crs=\"EPSG:4326\").to_crs(\"EPSG:2263\")\n",
    "\n",
    "gdf_parks = gpd.read_file(\"./data/Parks_Properties_20250123.kml\").to_crs(\"EPSG:2263\")\n",
    "\n",
    "# Find largest cluster center for Manhattan, Bronx\n",
    "gdf_manhattan = gdf_buildings[gdf_buildings[\"BoroName\"]==\"Manhattan\"]\n",
    "gdf_bronx = gdf_buildings[gdf_buildings[\"BoroName\"]==\"Bronx\"]\n",
    "\n",
    "print(gdf_manhattan)\n",
    "print(gdf_bronx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05465828-44e0-4c12-956e-35bb6de56dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronx x y:\n",
      "1017485.8860241318\n",
      "241760.33727313532\n",
      "manhattan x y:\n",
      "997790.3277237965\n",
      "232244.58317053123\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import MultiPoint\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def find_largest_building_cluster_center(gdf_bld, eps_m=200, min_samples=10):\n",
    "    centroids = gdf_bld.geometry.centroid\n",
    "    coords = np.column_stack([centroids.x, centroids.y])\n",
    "    db = DBSCAN(eps=eps_m, min_samples=min_samples).fit(coords)\n",
    "    labels = db.labels_\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    valid_mask = (unique >= 0)\n",
    "    if not np.any(valid_mask):\n",
    "        mp = MultiPoint(coords)\n",
    "        return mp.centroid\n",
    "    valid_labels = unique[valid_mask]\n",
    "    valid_counts = counts[valid_mask]\n",
    "    largest_id = valid_labels[np.argmax(valid_counts)]\n",
    "    in_largest = coords[labels==largest_id]\n",
    "    if len(in_largest)==0:\n",
    "        return MultiPoint(coords).centroid\n",
    "    else:\n",
    "        return MultiPoint(in_largest).centroid\n",
    "\n",
    "man_cent = find_largest_building_cluster_center(gdf_manhattan, eps_m=200, min_samples=20)\n",
    "brx_cent = find_largest_building_cluster_center(gdf_bronx, eps_m=200, min_samples=20)\n",
    "manhattan_x, manhattan_y = man_cent.x, man_cent.y\n",
    "bronx_x, bronx_y = brx_cent.x, brx_cent.y\n",
    "\n",
    "print(\"bronx x y:\")\n",
    "print(bronx_x)\n",
    "print(bronx_y)\n",
    "\n",
    "print(\"manhattan x y:\")\n",
    "print(manhattan_x)\n",
    "print(manhattan_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6952906c-17fb-4be8-a162-bf8b8b2b4efb",
   "metadata": {},
   "source": [
    "### LOAD + CHECK RASTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "611e885c-dc06-46a4-8eec-3604537de571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading reprojected LST + Indices...\n",
      "LST valid ratio: 1.000\n",
      "Indices valid ratio: 0.998\n",
      "LST raster bounds: (981437.4489166049, 212457.54099916507, 1023088.2146540903, 260009.54459268012)\n",
      "Indices raster bounds: (981462.3462672028, 212512.28574393364, 1023050.6163433938, 259944.0070645518)\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading reprojected LST + Indices...\")\n",
    "lst_raster = rxr.open_rasterio(\"Landsat_LST_v4_single_0601_0901.tiff\")\n",
    "lst_raster_2263 = lst_raster.rio.reproject(\"EPSG:2263\")\n",
    "\n",
    "indices_raster = rxr.open_rasterio(\"S2_indices_v4_single_0601_0901.tiff\")\n",
    "indices_raster_2263 = indices_raster.rio.reproject(\"EPSG:2263\")\n",
    "\n",
    "# Check if the rasters actually have valid data\n",
    "ratio_lst = (~lst_raster_2263.isnull()).mean().values\n",
    "ratio_idx = (~indices_raster_2263.isnull()).mean().values\n",
    "print(f\"LST valid ratio: {ratio_lst:.3f}\")\n",
    "print(f\"Indices valid ratio: {ratio_idx:.3f}\")\n",
    "\n",
    "if ratio_lst == 0.0:\n",
    "    print(\"WARNING: LST raster is entirely NaN. Possibly an empty mosaic or over-strict cloud mask!\")\n",
    "if ratio_idx == 0.0:\n",
    "    print(\"WARNING: Indices raster is entirely NaN. Possibly an empty mosaic or over-strict cloud mask!\")\n",
    "\n",
    "print(\"LST raster bounds:\", lst_raster_2263.rio.bounds())\n",
    "print(\"Indices raster bounds:\", indices_raster_2263.rio.bounds())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd83160e-a888-4e6c-9903-c915f3c1f23c",
   "metadata": {},
   "source": [
    "### BUILD TRAINING FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "002b2d3c-e428-45ee-b56a-d0248cc25852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"./data/Training_data_uhi_index_UHI2025-v2.csv\")\n",
    "gdf_train = gpd.GeoDataFrame(\n",
    "    df_train,\n",
    "    geometry=[Point(lon, lat) for lon, lat in zip(df_train.Longitude, df_train.Latitude)],\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(\"EPSG:2263\")\n",
    "\n",
    "cov_50, cov_100, cov_200 = [], [], []\n",
    "dist_w, dist_p = [], []\n",
    "lst_vals, ndvi_vals, ndbi_vals, ndwi_vals, evi_vals = [], [], [], [], []\n",
    "dist_man_vals, dist_bron_vals = [], []\n",
    "\n",
    "for i, row in gdf_train.iterrows():\n",
    "    geom = row.geometry\n",
    "\n",
    "    if FEATURE_FLAGS[\"building_cov_50m\"]:\n",
    "        cov_50.append(building_coverage_fraction(geom, gdf_buildings, 50))\n",
    "    else:\n",
    "        cov_50.append(0)\n",
    "    if FEATURE_FLAGS[\"building_cov_100m\"]:\n",
    "        cov_100.append(building_coverage_fraction(geom, gdf_buildings, 100))\n",
    "    else:\n",
    "        cov_100.append(0)\n",
    "    if FEATURE_FLAGS[\"building_cov_200m\"]:\n",
    "        cov_200.append(building_coverage_fraction(geom, gdf_buildings, 200))\n",
    "    else:\n",
    "        cov_200.append(0)\n",
    "\n",
    "    # water / parks\n",
    "    if FEATURE_FLAGS[\"distance_water\"]:\n",
    "        dist_w.append(distance_to_polygons(geom, gdf_water))\n",
    "    else:\n",
    "        dist_w.append(0)\n",
    "    if FEATURE_FLAGS[\"distance_parks\"]:\n",
    "        dist_p.append(distance_to_polygons(geom, gdf_parks))\n",
    "    else:\n",
    "        dist_p.append(0)\n",
    "\n",
    "    # LST\n",
    "    lv = 0\n",
    "    if FEATURE_FLAGS[\"lst_value\"]:\n",
    "        lv = extract_raster_value(geom, lst_raster_2263, band_index=1)\n",
    "    lst_vals.append(lv)\n",
    "\n",
    "    # NDVI / NDBI / NDWI\n",
    "    ndv, ndb, ndw = 0,0,0\n",
    "    if FEATURE_FLAGS[\"ndvi_value\"]:\n",
    "        ndv = extract_raster_value(geom, indices_raster_2263, 1)\n",
    "    if FEATURE_FLAGS[\"ndbi_value\"]:\n",
    "        ndb = extract_raster_value(geom, indices_raster_2263, 2)\n",
    "    if FEATURE_FLAGS[\"ndwi_value\"]:\n",
    "        ndw = extract_raster_value(geom, indices_raster_2263, 3)\n",
    "    ndvi_vals.append(ndv)\n",
    "    ndbi_vals.append(ndb)\n",
    "    ndwi_vals.append(ndw)\n",
    "\n",
    "    # EVI\n",
    "    evi_val = 0\n",
    "    if FEATURE_FLAGS[\"evi_value\"]:\n",
    "        # e.g. blue=band1, red=band3, nir=band4 in another raster\n",
    "        # evi_val = compute_evi(blue, red, nir)\n",
    "        pass\n",
    "    evi_vals.append(evi_val)\n",
    "\n",
    "    # Dist manhattan / bronx\n",
    "    distm = 0\n",
    "    if FEATURE_FLAGS[\"dist_manhattan_centre\"]:\n",
    "        distm = euclidean_distance(geom.x, geom.y, manhattan_x, manhattan_y)\n",
    "    distb = 0\n",
    "    if FEATURE_FLAGS[\"dist_bronx_centre\"]:\n",
    "        distb = euclidean_distance(geom.x, geom.y, bronx_x, bronx_y)\n",
    "\n",
    "    dist_man_vals.append(distm)\n",
    "    dist_bron_vals.append(distb)\n",
    "\n",
    "gdf_train[\"bld_cov_50\"] = cov_50\n",
    "gdf_train[\"bld_cov_100\"] = cov_100\n",
    "gdf_train[\"bld_cov_200\"] = cov_200\n",
    "gdf_train[\"dist_water\"] = dist_w\n",
    "gdf_train[\"dist_parks\"] = dist_p\n",
    "gdf_train[\"lst_value\"] = lst_vals\n",
    "gdf_train[\"ndvi_value\"] = ndvi_vals\n",
    "gdf_train[\"ndbi_value\"] = ndbi_vals\n",
    "gdf_train[\"ndwi_value\"] = ndwi_vals\n",
    "# gdf_train[\"evi_value\"] = evi_vals\n",
    "gdf_train[\"dist_manh\"] = dist_man_vals\n",
    "gdf_train[\"dist_bron\"] = dist_bron_vals\n",
    "\n",
    "# optional KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "if FEATURE_FLAGS[\"location_cluster\"]:\n",
    "    coords_train = np.column_stack([gdf_train.geometry.x, gdf_train.geometry.y])\n",
    "    kmeans = KMeans(n_clusters=8, random_state=RANDOM_SEED, n_init=10).fit(coords_train)\n",
    "    gdf_train[\"location_cluster\"] = kmeans.labels_\n",
    "else:\n",
    "    kmeans = None\n",
    "    gdf_train[\"location_cluster\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6a8bc06-2e0d-4098-af2d-dbee9857a840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (11229, 12)\n",
      "Number of NaN in X? 0\n"
     ]
    }
   ],
   "source": [
    "all_features = [\n",
    "    \"bld_cov_50\", \"bld_cov_100\", \"bld_cov_200\",\n",
    "    \"dist_water\", \"dist_parks\",\n",
    "    \"lst_value\", \"ndvi_value\", \"ndbi_value\", \"ndwi_value\", \n",
    "    \"dist_manh\", \"dist_bron\",\n",
    "    \"location_cluster\"\n",
    "]\n",
    "\n",
    "df_train_feat = gdf_train[all_features].fillna(0.0)\n",
    "X = df_train_feat.values\n",
    "y = gdf_train[\"UHI Index\"].values\n",
    "print(\"Train shape:\", X.shape)\n",
    "print(\"Number of NaN in X?\", np.isnan(X).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caa84f51-fa94-4968-a169-b7e196797da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>datetime</th>\n",
       "      <th>UHI Index</th>\n",
       "      <th>geometry</th>\n",
       "      <th>bld_cov_50</th>\n",
       "      <th>bld_cov_100</th>\n",
       "      <th>bld_cov_200</th>\n",
       "      <th>dist_water</th>\n",
       "      <th>dist_parks</th>\n",
       "      <th>lst_value</th>\n",
       "      <th>ndvi_value</th>\n",
       "      <th>ndbi_value</th>\n",
       "      <th>ndwi_value</th>\n",
       "      <th>dist_manh</th>\n",
       "      <th>dist_bron</th>\n",
       "      <th>location_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.909167</td>\n",
       "      <td>40.813107</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.030289</td>\n",
       "      <td>POINT (1009393.606 235526.824)</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>0.119473</td>\n",
       "      <td>0.248509</td>\n",
       "      <td>3536.717021</td>\n",
       "      <td>371.277131</td>\n",
       "      <td>41.442815</td>\n",
       "      <td>-0.032849</td>\n",
       "      <td>-0.027607</td>\n",
       "      <td>0.044681</td>\n",
       "      <td>12058.572295</td>\n",
       "      <td>10214.777740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.909187</td>\n",
       "      <td>40.813045</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.030289</td>\n",
       "      <td>POINT (1009388.093 235504.35)</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.159338</td>\n",
       "      <td>0.260397</td>\n",
       "      <td>3520.867715</td>\n",
       "      <td>358.889116</td>\n",
       "      <td>41.442815</td>\n",
       "      <td>-0.017685</td>\n",
       "      <td>-0.025670</td>\n",
       "      <td>0.039056</td>\n",
       "      <td>12047.167026</td>\n",
       "      <td>10232.870157</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-73.909215</td>\n",
       "      <td>40.812978</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.023798</td>\n",
       "      <td>POINT (1009380.276 235480.052)</td>\n",
       "      <td>0.026731</td>\n",
       "      <td>0.194538</td>\n",
       "      <td>0.278762</td>\n",
       "      <td>3504.846309</td>\n",
       "      <td>344.200944</td>\n",
       "      <td>41.442815</td>\n",
       "      <td>-0.023479</td>\n",
       "      <td>-0.024851</td>\n",
       "      <td>0.039874</td>\n",
       "      <td>12033.085822</td>\n",
       "      <td>10253.921329</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.909242</td>\n",
       "      <td>40.812908</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.023798</td>\n",
       "      <td>POINT (1009372.92 235454.541)</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.227561</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>3487.673871</td>\n",
       "      <td>330.119247</td>\n",
       "      <td>41.152283</td>\n",
       "      <td>-0.024157</td>\n",
       "      <td>-0.027820</td>\n",
       "      <td>0.041210</td>\n",
       "      <td>12019.162378</td>\n",
       "      <td>10275.373165</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.909257</td>\n",
       "      <td>40.812845</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.021634</td>\n",
       "      <td>POINT (1009368.791 235431.463)</td>\n",
       "      <td>0.049321</td>\n",
       "      <td>0.263256</td>\n",
       "      <td>0.351619</td>\n",
       "      <td>3470.826136</td>\n",
       "      <td>320.290749</td>\n",
       "      <td>41.152283</td>\n",
       "      <td>-0.024157</td>\n",
       "      <td>-0.027820</td>\n",
       "      <td>0.041210</td>\n",
       "      <td>12009.039312</td>\n",
       "      <td>10292.806926</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11224</th>\n",
       "      <td>-73.957050</td>\n",
       "      <td>40.790333</td>\n",
       "      <td>24-07-2021 15:57</td>\n",
       "      <td>0.972470</td>\n",
       "      <td>POINT (996143.074 227219.577)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>569.780991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.890471</td>\n",
       "      <td>0.028827</td>\n",
       "      <td>-0.023243</td>\n",
       "      <td>-0.000817</td>\n",
       "      <td>5288.111794</td>\n",
       "      <td>25825.361761</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11225</th>\n",
       "      <td>-73.957063</td>\n",
       "      <td>40.790308</td>\n",
       "      <td>24-07-2021 15:57</td>\n",
       "      <td>0.972470</td>\n",
       "      <td>POINT (996139.388 227210.467)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>559.973656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.890471</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>-0.028651</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>5297.917140</td>\n",
       "      <td>25833.538520</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11226</th>\n",
       "      <td>-73.957093</td>\n",
       "      <td>40.790270</td>\n",
       "      <td>24-07-2021 15:57</td>\n",
       "      <td>0.981124</td>\n",
       "      <td>POINT (996131.087 227196.498)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>543.787136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.890471</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>-0.028651</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>5313.778307</td>\n",
       "      <td>25848.265634</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11227</th>\n",
       "      <td>-73.957112</td>\n",
       "      <td>40.790253</td>\n",
       "      <td>24-07-2021 15:59</td>\n",
       "      <td>0.981245</td>\n",
       "      <td>POINT (996126.012 227190.422)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>536.117477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.890471</td>\n",
       "      <td>0.048808</td>\n",
       "      <td>-0.048659</td>\n",
       "      <td>-0.018743</td>\n",
       "      <td>5321.136077</td>\n",
       "      <td>25855.882277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11228</th>\n",
       "      <td>-73.957128</td>\n",
       "      <td>40.790237</td>\n",
       "      <td>24-07-2021 15:59</td>\n",
       "      <td>0.983408</td>\n",
       "      <td>POINT (996121.402 227184.35)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>528.656834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>34.890471</td>\n",
       "      <td>0.048808</td>\n",
       "      <td>-0.048659</td>\n",
       "      <td>-0.018743</td>\n",
       "      <td>5328.346048</td>\n",
       "      <td>25863.112636</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11229 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Longitude   Latitude          datetime  UHI Index  \\\n",
       "0     -73.909167  40.813107  24-07-2021 15:53   1.030289   \n",
       "1     -73.909187  40.813045  24-07-2021 15:53   1.030289   \n",
       "2     -73.909215  40.812978  24-07-2021 15:53   1.023798   \n",
       "3     -73.909242  40.812908  24-07-2021 15:53   1.023798   \n",
       "4     -73.909257  40.812845  24-07-2021 15:53   1.021634   \n",
       "...          ...        ...               ...        ...   \n",
       "11224 -73.957050  40.790333  24-07-2021 15:57   0.972470   \n",
       "11225 -73.957063  40.790308  24-07-2021 15:57   0.972470   \n",
       "11226 -73.957093  40.790270  24-07-2021 15:57   0.981124   \n",
       "11227 -73.957112  40.790253  24-07-2021 15:59   0.981245   \n",
       "11228 -73.957128  40.790237  24-07-2021 15:59   0.983408   \n",
       "\n",
       "                             geometry  bld_cov_50  bld_cov_100  bld_cov_200  \\\n",
       "0      POINT (1009393.606 235526.824)    0.019848     0.119473     0.248509   \n",
       "1       POINT (1009388.093 235504.35)    0.039795     0.159338     0.260397   \n",
       "2      POINT (1009380.276 235480.052)    0.026731     0.194538     0.278762   \n",
       "3       POINT (1009372.92 235454.541)    0.020690     0.227561     0.312600   \n",
       "4      POINT (1009368.791 235431.463)    0.049321     0.263256     0.351619   \n",
       "...                               ...         ...          ...          ...   \n",
       "11224   POINT (996143.074 227219.577)    0.000000     0.000000     0.000000   \n",
       "11225   POINT (996139.388 227210.467)    0.000000     0.000000     0.000000   \n",
       "11226   POINT (996131.087 227196.498)    0.000000     0.000000     0.000000   \n",
       "11227   POINT (996126.012 227190.422)    0.000000     0.000000     0.000000   \n",
       "11228    POINT (996121.402 227184.35)    0.000000     0.000000     0.000000   \n",
       "\n",
       "        dist_water  dist_parks  lst_value  ndvi_value  ndbi_value  ndwi_value  \\\n",
       "0      3536.717021  371.277131  41.442815   -0.032849   -0.027607    0.044681   \n",
       "1      3520.867715  358.889116  41.442815   -0.017685   -0.025670    0.039056   \n",
       "2      3504.846309  344.200944  41.442815   -0.023479   -0.024851    0.039874   \n",
       "3      3487.673871  330.119247  41.152283   -0.024157   -0.027820    0.041210   \n",
       "4      3470.826136  320.290749  41.152283   -0.024157   -0.027820    0.041210   \n",
       "...            ...         ...        ...         ...         ...         ...   \n",
       "11224   569.780991    0.000000  34.890471    0.028827   -0.023243   -0.000817   \n",
       "11225   559.973656    0.000000  34.890471    0.051500   -0.028651   -0.016411   \n",
       "11226   543.787136    0.000000  34.890471    0.051500   -0.028651   -0.016411   \n",
       "11227   536.117477    0.000000  34.890471    0.048808   -0.048659   -0.018743   \n",
       "11228   528.656834    0.000000  34.890471    0.048808   -0.048659   -0.018743   \n",
       "\n",
       "          dist_manh     dist_bron  location_cluster  \n",
       "0      12058.572295  10214.777740                 2  \n",
       "1      12047.167026  10232.870157                 2  \n",
       "2      12033.085822  10253.921329                 2  \n",
       "3      12019.162378  10275.373165                 2  \n",
       "4      12009.039312  10292.806926                 2  \n",
       "...             ...           ...               ...  \n",
       "11224   5288.111794  25825.361761                 0  \n",
       "11225   5297.917140  25833.538520                 0  \n",
       "11226   5313.778307  25848.265634                 0  \n",
       "11227   5321.136077  25855.882277                 0  \n",
       "11228   5328.346048  25863.112636                 0  \n",
       "\n",
       "[11229 rows x 17 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31a36e2e-9398-4086-89da-78078fce195c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>UHI Index</th>\n",
       "      <th>bld_cov_50</th>\n",
       "      <th>bld_cov_100</th>\n",
       "      <th>bld_cov_200</th>\n",
       "      <th>dist_water</th>\n",
       "      <th>dist_parks</th>\n",
       "      <th>lst_value</th>\n",
       "      <th>ndvi_value</th>\n",
       "      <th>ndbi_value</th>\n",
       "      <th>ndwi_value</th>\n",
       "      <th>dist_manh</th>\n",
       "      <th>dist_bron</th>\n",
       "      <th>location_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "      <td>11229.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-73.933927</td>\n",
       "      <td>40.808800</td>\n",
       "      <td>1.000001</td>\n",
       "      <td>0.125190</td>\n",
       "      <td>0.228201</td>\n",
       "      <td>0.303530</td>\n",
       "      <td>1891.184531</td>\n",
       "      <td>362.798049</td>\n",
       "      <td>40.654181</td>\n",
       "      <td>-0.006387</td>\n",
       "      <td>-0.002206</td>\n",
       "      <td>0.023604</td>\n",
       "      <td>11321.153684</td>\n",
       "      <td>18163.913620</td>\n",
       "      <td>3.224686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.028253</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>0.016238</td>\n",
       "      <td>0.175080</td>\n",
       "      <td>0.185421</td>\n",
       "      <td>0.188678</td>\n",
       "      <td>1298.246512</td>\n",
       "      <td>360.343364</td>\n",
       "      <td>2.714054</td>\n",
       "      <td>0.062528</td>\n",
       "      <td>0.065959</td>\n",
       "      <td>0.047172</td>\n",
       "      <td>5453.390526</td>\n",
       "      <td>9320.532698</td>\n",
       "      <td>2.338326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-73.994457</td>\n",
       "      <td>40.758792</td>\n",
       "      <td>0.956122</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.933619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.484185</td>\n",
       "      <td>-0.231795</td>\n",
       "      <td>-0.353383</td>\n",
       "      <td>-0.499395</td>\n",
       "      <td>146.773827</td>\n",
       "      <td>2863.788821</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-73.955703</td>\n",
       "      <td>40.790905</td>\n",
       "      <td>0.988577</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058977</td>\n",
       "      <td>0.166868</td>\n",
       "      <td>930.568027</td>\n",
       "      <td>76.262961</td>\n",
       "      <td>39.227938</td>\n",
       "      <td>-0.037236</td>\n",
       "      <td>-0.036320</td>\n",
       "      <td>0.015558</td>\n",
       "      <td>6642.556117</td>\n",
       "      <td>9804.072416</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-73.932968</td>\n",
       "      <td>40.810688</td>\n",
       "      <td>1.000237</td>\n",
       "      <td>0.048059</td>\n",
       "      <td>0.215740</td>\n",
       "      <td>0.311991</td>\n",
       "      <td>1617.923389</td>\n",
       "      <td>284.983308</td>\n",
       "      <td>40.868588</td>\n",
       "      <td>-0.019680</td>\n",
       "      <td>-0.004430</td>\n",
       "      <td>0.032591</td>\n",
       "      <td>11533.755497</td>\n",
       "      <td>17965.861358</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-73.909647</td>\n",
       "      <td>40.824515</td>\n",
       "      <td>1.011176</td>\n",
       "      <td>0.198378</td>\n",
       "      <td>0.350740</td>\n",
       "      <td>0.435660</td>\n",
       "      <td>2415.031074</td>\n",
       "      <td>505.198800</td>\n",
       "      <td>42.341754</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>0.030177</td>\n",
       "      <td>0.045274</td>\n",
       "      <td>15730.919401</td>\n",
       "      <td>25630.341184</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>-73.879458</td>\n",
       "      <td>40.859497</td>\n",
       "      <td>1.046036</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.999675</td>\n",
       "      <td>0.824851</td>\n",
       "      <td>6041.065986</td>\n",
       "      <td>1998.456627</td>\n",
       "      <td>54.564594</td>\n",
       "      <td>0.562697</td>\n",
       "      <td>0.440744</td>\n",
       "      <td>0.165486</td>\n",
       "      <td>21732.239259</td>\n",
       "      <td>38257.009798</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Longitude      Latitude     UHI Index    bld_cov_50   bld_cov_100  \\\n",
       "count  11229.000000  11229.000000  11229.000000  11229.000000  11229.000000   \n",
       "mean     -73.933927     40.808800      1.000001      0.125190      0.228201   \n",
       "std        0.028253      0.023171      0.016238      0.175080      0.185421   \n",
       "min      -73.994457     40.758792      0.956122      0.000000      0.000000   \n",
       "25%      -73.955703     40.790905      0.988577      0.000000      0.058977   \n",
       "50%      -73.932968     40.810688      1.000237      0.048059      0.215740   \n",
       "75%      -73.909647     40.824515      1.011176      0.198378      0.350740   \n",
       "max      -73.879458     40.859497      1.046036      1.000000      0.999675   \n",
       "\n",
       "        bld_cov_200    dist_water    dist_parks     lst_value    ndvi_value  \\\n",
       "count  11229.000000  11229.000000  11229.000000  11229.000000  11229.000000   \n",
       "mean       0.303530   1891.184531    362.798049     40.654181     -0.006387   \n",
       "std        0.188678   1298.246512    360.343364      2.714054      0.062528   \n",
       "min        0.000000     20.933619      0.000000     32.484185     -0.231795   \n",
       "25%        0.166868    930.568027     76.262961     39.227938     -0.037236   \n",
       "50%        0.311991   1617.923389    284.983308     40.868588     -0.019680   \n",
       "75%        0.435660   2415.031074    505.198800     42.341754      0.006772   \n",
       "max        0.824851   6041.065986   1998.456627     54.564594      0.562697   \n",
       "\n",
       "         ndbi_value    ndwi_value     dist_manh     dist_bron  \\\n",
       "count  11229.000000  11229.000000  11229.000000  11229.000000   \n",
       "mean      -0.002206      0.023604  11321.153684  18163.913620   \n",
       "std        0.065959      0.047172   5453.390526   9320.532698   \n",
       "min       -0.353383     -0.499395    146.773827   2863.788821   \n",
       "25%       -0.036320      0.015558   6642.556117   9804.072416   \n",
       "50%       -0.004430      0.032591  11533.755497  17965.861358   \n",
       "75%        0.030177      0.045274  15730.919401  25630.341184   \n",
       "max        0.440744      0.165486  21732.239259  38257.009798   \n",
       "\n",
       "       location_cluster  \n",
       "count      11229.000000  \n",
       "mean           3.224686  \n",
       "std            2.338326  \n",
       "min            0.000000  \n",
       "25%            1.000000  \n",
       "50%            3.000000  \n",
       "75%            5.000000  \n",
       "max            7.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf2817-a9ca-451f-94db-1e077e180047",
   "metadata": {},
   "source": [
    "### Final Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c73a2f73-5eaa-42a5-9c02-76d50dfecbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train feature shape: (11229, 12)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"bld_cov_50\", \"bld_cov_100\", \"bld_cov_200\",\n",
    "    \"dist_water\", \"dist_parks\",\n",
    "    \"lst_value\",\n",
    "    \"ndvi_value\", \"ndbi_value\", \"ndwi_value\", \n",
    "    \"dist_manh\", \"dist_bron\",\n",
    "    \"location_cluster\"\n",
    "]\n",
    "X = gdf_train[feature_cols].fillna(0.0).values\n",
    "y = gdf_train[\"UHI Index\"].values\n",
    "print(\"Final train feature shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216e236-38bd-4d5f-a090-fc263396544d",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b327326e-7313-42f5-adbe-d0656c6ef1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stratified_bins(target, n_bins=10):\n",
    "    \"\"\"Bin the continuous target into discrete intervals for use in 'StratifiedKFold'.\"\"\"\n",
    "    # pd.qcut => quantile-based discretization\n",
    "    bins = pd.qcut(target, q=n_bins, duplicates=\"drop\")  # If duplicates occur, drop them\n",
    "    return bins.astype(str)  # Convert to string labels\n",
    "\n",
    "K_FOLDS = 10\n",
    "\n",
    "y_bins = make_stratified_bins(y, n_bins=10)\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "# kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75bda76a-4300-41b2-a859-b1f3acd94841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions and parameter grids\n",
    "models_and_params = {\n",
    "    \"RandomForest\": (\n",
    "        RandomForestRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200, 500],\n",
    "            \"max_depth\": [None],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "            \"min_samples_split\": [4, 5, 6],\n",
    "        }\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"n_estimators\": [500, 1000, 2000],\n",
    "            \"max_depth\": [None, 10, 20, 40, 50, 60],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "        }\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"max_depth\": [10, 20, 40],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "        }\n",
    "    ),\n",
    "    \"KNeighbors\": (\n",
    "        KNeighborsRegressor(),\n",
    "        {\n",
    "            \"n_neighbors\": [2, 3, 4, 5, 10],\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "            \"p\": [1, 2, 3, 4, 5],  # Manhattan or Euclidean distance\n",
    "            \"algorithm\": ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            \"leaf_size\": [10, 20, 30, 40],\n",
    "            \"metric\": ['minkowski', 'euclidean', 'manhattan', 'chebyshev'],\n",
    "        }\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        XGBRegressor(random_state=RANDOM_SEED, eval_metric=\"rmse\", use_label_encoder=False),\n",
    "        {\n",
    "            \"n_estimators\": [500, 1000, 2000],\n",
    "            \"max_depth\": [15, 20, 25],\n",
    "            \"learning_rate\": [0.005, 0.01, 0.02],\n",
    "            \"subsample\": [0.5, 0.6, 0.7],\n",
    "            \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        lgb.LGBMRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"n_estimators\": [100, 200, 500],\n",
    "            \"max_depth\": [10, 20, 50],\n",
    "            \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "            \"subsample\": [0.6, 0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "        }\n",
    "    ),\n",
    "    \"CatBoost\": (\n",
    "        CatBoostRegressor(silent=True, random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"iterations\": [100, 200, 500],\n",
    "            \"depth\": [2, 5, 10],\n",
    "            \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "            \"random_strength\": [1, 2, 3, 5],\n",
    "        }\n",
    "    ),\n",
    "    \"HistGradientBoosting\": (\n",
    "        HistGradientBoostingRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"max_iter\": [100, 200, 500],\n",
    "            \"learning_rate\": [0.001, 0.01, 0.05, 0.1],\n",
    "            \"max_depth\": [None, 5, 10, 20],\n",
    "            \"max_leaf_nodes\": [15, 31, 63],\n",
    "            \"min_samples_leaf\": [10, 20, 50],\n",
    "            \"l2_regularization\": [0.0, 0.1, 1.0]\n",
    "        }\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c60c60b9-dcc8-4934-a4b7-7b3d0b392551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Searching RandomForest ===\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "\n",
      "=== Searching ExtraTrees ===\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "\n",
      "=== Searching DecisionTree ===\n",
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "\n",
      "=== Searching KNeighbors ===\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "20 fits failed out of a total of 200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\neighbors\\_regression.py\", line 218, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"C:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 500, in _fit\n",
      "    self._check_algorithm_metric()\n",
      "  File \"C:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 434, in _check_algorithm_metric\n",
      "    raise ValueError(\n",
      "ValueError: Metric 'cosine' not valid. Use sorted(sklearn.neighbors.VALID_METRICS['ball_tree']) to get valid options. Metric can also be a callable function.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Searching XGBoost ===\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "\n",
      "=== Searching LightGBM ===\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2813\n",
      "[LightGBM] [Info] Number of data points in the train set: 11229, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 1.000001\n",
      "\n",
      "=== Searching CatBoost ===\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "\n",
      "=== Searching HistGradientBoosting ===\n",
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n",
      "\n",
      "Final Cross-Val Results:\n",
      "                   Model                                     Best Estimator  \\\n",
      "3            KNeighbors  KNeighborsRegressor(algorithm='brute', n_neigh...   \n",
      "1            ExtraTrees  (ExtraTreeRegressor(max_depth=50, random_state...   \n",
      "4               XGBoost  XGBRegressor(base_score=None, booster=None, ca...   \n",
      "0          RandomForest  (DecisionTreeRegressor(max_features=1.0, min_s...   \n",
      "5              LightGBM  LGBMRegressor(colsample_bytree=0.9, learning_r...   \n",
      "6              CatBoost  <catboost.core.CatBoostRegressor object at 0x0...   \n",
      "7  HistGradientBoosting  HistGradientBoostingRegressor(l2_regularizatio...   \n",
      "2          DecisionTree  DecisionTreeRegressor(max_depth=20, random_sta...   \n",
      "\n",
      "   Best Score (CV)                                        Best Params  \n",
      "3         0.978279  {'weights': 'distance', 'p': 4, 'n_neighbors':...  \n",
      "1         0.974335  {'n_estimators': 1000, 'min_samples_leaf': 1, ...  \n",
      "4         0.966350  {'subsample': 0.7, 'n_estimators': 2000, 'max_...  \n",
      "0         0.960758  {'n_estimators': 500, 'min_samples_split': 4, ...  \n",
      "5         0.953160  {'subsample': 0.8, 'n_estimators': 500, 'max_d...  \n",
      "6         0.950086  {'random_strength': 3, 'learning_rate': 0.2, '...  \n",
      "7         0.937012  {'min_samples_leaf': 10, 'max_leaf_nodes': 63,...  \n",
      "2         0.913740           {'min_samples_leaf': 1, 'max_depth': 20}  \n"
     ]
    }
   ],
   "source": [
    "N_ITER = 20\n",
    "results = []\n",
    "\n",
    "for model_name, (model, param_grid) in models_and_params.items():\n",
    "    print(f\"\\n=== Searching {model_name} ===\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=N_ITER,\n",
    "        cv=list(skf.split(X, y_bins)),\n",
    "        scoring=\"r2\",\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    best_estimator = search.best_estimator_\n",
    "    best_score = search.best_score_\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Best Estimator\": best_estimator,\n",
    "        \"Best Score (CV)\": best_score,\n",
    "        \"Best Params\": best_params\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Best Score (CV)\", ascending=False)\n",
    "print(\"\\nFinal Cross-Val Results:\\n\", results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018bd501-3c56-42be-a688-c3546e79dea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Estimator</th>\n",
       "      <th>Best Score (CV)</th>\n",
       "      <th>Best Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>KNeighborsRegressor(algorithm='brute', n_neigh...</td>\n",
       "      <td>0.978279</td>\n",
       "      <td>{'weights': 'distance', 'p': 4, 'n_neighbors':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>(ExtraTreeRegressor(max_depth=50, random_state...</td>\n",
       "      <td>0.974335</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_leaf': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>XGBRegressor(base_score=None, booster=None, ca...</td>\n",
       "      <td>0.966350</td>\n",
       "      <td>{'subsample': 0.7, 'n_estimators': 2000, 'max_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>(DecisionTreeRegressor(max_features=1.0, min_s...</td>\n",
       "      <td>0.960758</td>\n",
       "      <td>{'n_estimators': 500, 'min_samples_split': 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>LGBMRegressor(colsample_bytree=0.9, learning_r...</td>\n",
       "      <td>0.953160</td>\n",
       "      <td>{'subsample': 0.8, 'n_estimators': 500, 'max_d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>&lt;catboost.core.CatBoostRegressor object at 0x0...</td>\n",
       "      <td>0.950086</td>\n",
       "      <td>{'random_strength': 3, 'learning_rate': 0.2, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HistGradientBoosting</td>\n",
       "      <td>HistGradientBoostingRegressor(l2_regularizatio...</td>\n",
       "      <td>0.937012</td>\n",
       "      <td>{'min_samples_leaf': 10, 'max_leaf_nodes': 63,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=20, random_sta...</td>\n",
       "      <td>0.913740</td>\n",
       "      <td>{'min_samples_leaf': 1, 'max_depth': 20}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model                                     Best Estimator  \\\n",
       "3            KNeighbors  KNeighborsRegressor(algorithm='brute', n_neigh...   \n",
       "1            ExtraTrees  (ExtraTreeRegressor(max_depth=50, random_state...   \n",
       "4               XGBoost  XGBRegressor(base_score=None, booster=None, ca...   \n",
       "0          RandomForest  (DecisionTreeRegressor(max_features=1.0, min_s...   \n",
       "5              LightGBM  LGBMRegressor(colsample_bytree=0.9, learning_r...   \n",
       "6              CatBoost  <catboost.core.CatBoostRegressor object at 0x0...   \n",
       "7  HistGradientBoosting  HistGradientBoostingRegressor(l2_regularizatio...   \n",
       "2          DecisionTree  DecisionTreeRegressor(max_depth=20, random_sta...   \n",
       "\n",
       "   Best Score (CV)                                        Best Params  \n",
       "3         0.978279  {'weights': 'distance', 'p': 4, 'n_neighbors':...  \n",
       "1         0.974335  {'n_estimators': 1000, 'min_samples_leaf': 1, ...  \n",
       "4         0.966350  {'subsample': 0.7, 'n_estimators': 2000, 'max_...  \n",
       "0         0.960758  {'n_estimators': 500, 'min_samples_split': 4, ...  \n",
       "5         0.953160  {'subsample': 0.8, 'n_estimators': 500, 'max_d...  \n",
       "6         0.950086  {'random_strength': 3, 'learning_rate': 0.2, '...  \n",
       "7         0.937012  {'min_samples_leaf': 10, 'max_leaf_nodes': 63,...  \n",
       "2         0.913740           {'min_samples_leaf': 1, 'max_depth': 20}  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.to_csv('results_df.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a236c4f-6c3d-4bbe-b454-6001ed5ae71b",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11add13f-bff5-4cf2-8b1b-554334251d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation shape: (1040, 12)\n"
     ]
    }
   ],
   "source": [
    "# 4) Build Validation Data\n",
    "df_val = pd.read_csv(\"./data/Submission_template_UHI2025-v2.csv\")\n",
    "gdf_val = gpd.GeoDataFrame(\n",
    "    df_val,\n",
    "    geometry=[Point(lon, lat) for lon, lat in zip(df_val.Longitude, df_val.Latitude)],\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(\"EPSG:2263\")\n",
    "\n",
    "# We'll produce the same set of columns for validation:\n",
    "cov_50_vals, cov_100_vals, cov_200_vals = [], [], []\n",
    "dist_w_vals, dist_p_vals = [], []\n",
    "lst_vals, ndvi_vals, ndbi_vals, ndwi_vals, evi_vals = [], [], [], [], []\n",
    "dist_manh_vals, dist_bron_vals = [], []\n",
    "\n",
    "for i, row in gdf_val.iterrows():\n",
    "    geom = row.geometry\n",
    "\n",
    "    # coverage\n",
    "    if FEATURE_FLAGS[\"building_cov_50m\"]:\n",
    "        cov_50_vals.append(building_coverage_fraction(geom, gdf_buildings, 50))\n",
    "    else:\n",
    "        cov_50_vals.append(0)\n",
    "\n",
    "    if FEATURE_FLAGS[\"building_cov_100m\"]:\n",
    "        cov_100_vals.append(building_coverage_fraction(geom, gdf_buildings, 100))\n",
    "    else:\n",
    "        cov_100_vals.append(0)\n",
    "\n",
    "    if FEATURE_FLAGS[\"building_cov_200m\"]:\n",
    "        cov_200_vals.append(building_coverage_fraction(geom, gdf_buildings, 200))\n",
    "    else:\n",
    "        cov_200_vals.append(0)\n",
    "\n",
    "    # dist water, parks\n",
    "    dist_w_vals.append(distance_to_polygons(geom, gdf_water) if FEATURE_FLAGS[\"distance_water\"] else 0)\n",
    "    dist_p_vals.append(distance_to_polygons(geom, gdf_parks) if FEATURE_FLAGS[\"distance_parks\"] else 0)\n",
    "\n",
    "    # LST\n",
    "    lv = 0\n",
    "    if FEATURE_FLAGS[\"lst_value\"]:\n",
    "        lv = extract_raster_value(geom, lst_raster_2263, 1)\n",
    "    lst_vals.append(lv)\n",
    "\n",
    "    # NDVI/NDBI/NDWI\n",
    "    ndv, ndb, ndw = 0,0,0\n",
    "    if FEATURE_FLAGS[\"ndvi_value\"]:\n",
    "        ndv = extract_raster_value(geom, indices_raster_2263, 1)\n",
    "    if FEATURE_FLAGS[\"ndbi_value\"]:\n",
    "        ndb = extract_raster_value(geom, indices_raster_2263, 2)\n",
    "    if FEATURE_FLAGS[\"ndwi_value\"]:\n",
    "        ndw = extract_raster_value(geom, indices_raster_2263, 3)\n",
    "    ndvi_vals.append(ndv)\n",
    "    ndbi_vals.append(ndb)\n",
    "    ndwi_vals.append(ndw)\n",
    "\n",
    "    # EVI (if needed)\n",
    "    evi_val = 0\n",
    "    if FEATURE_FLAGS[\"evi_value\"]:\n",
    "        # same logic as training\n",
    "        pass\n",
    "    evi_vals.append(evi_val)\n",
    "\n",
    "    # dist manhattan, bronx\n",
    "    distm = 0\n",
    "    if FEATURE_FLAGS[\"dist_manhattan_centre\"]:\n",
    "        distm = euclidean_distance(geom.x, geom.y, manhattan_x, manhattan_y)\n",
    "    distb = 0\n",
    "    if FEATURE_FLAGS[\"dist_bronx_centre\"]:\n",
    "        distb = euclidean_distance(geom.x, geom.y, bronx_x, bronx_y)\n",
    "    dist_manh_vals.append(distm)\n",
    "    dist_bron_vals.append(distb)\n",
    "\n",
    "df_val[\"bld_cov_50\"]  = cov_50_vals\n",
    "df_val[\"bld_cov_100\"] = cov_100_vals\n",
    "df_val[\"bld_cov_200\"] = cov_200_vals\n",
    "df_val[\"dist_water\"]  = dist_w_vals\n",
    "df_val[\"dist_parks\"]  = dist_p_vals\n",
    "df_val[\"lst_value\"]   = lst_vals\n",
    "df_val[\"ndvi_value\"]  = ndvi_vals\n",
    "df_val[\"ndbi_value\"]  = ndbi_vals\n",
    "df_val[\"ndwi_value\"]  = ndwi_vals\n",
    "# df_val[\"evi_value\"]   = evi_vals\n",
    "df_val[\"dist_manh\"]   = dist_manh_vals\n",
    "df_val[\"dist_bron\"]   = dist_bron_vals\n",
    "\n",
    "# location clusters if needed\n",
    "if FEATURE_FLAGS[\"location_cluster\"] and (\"kmeans\" in globals()) and kmeans is not None:\n",
    "    coords_val = np.column_stack([gdf_val.geometry.x, gdf_val.geometry.y])\n",
    "    df_val[\"location_cluster\"] = kmeans.predict(coords_val)\n",
    "else:\n",
    "    df_val[\"location_cluster\"] = 0\n",
    "\n",
    "df_val_feat = df_val[feature_cols].fillna(0.0)\n",
    "X_val = df_val_feat.values\n",
    "print(\"Validation shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99f3a93f-6c2b-45e7-9995-3eb039fe7314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Models:\n",
      " [('KNeighbors', KNeighborsRegressor(algorithm='brute', n_neighbors=2, p=4, weights='distance')), ('ExtraTrees', ExtraTreesRegressor(max_depth=50, n_estimators=1000, random_state=42)), ('XGBoost', XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric='rmse', feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=25, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=2000, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...))]\n"
     ]
    }
   ],
   "source": [
    "# Let's pick top 4 models from the results\n",
    "top3 = results_df.head(3).reset_index(drop=True)\n",
    "best_model_1 = top3.iloc[0][\"Best Estimator\"]\n",
    "best_model_2 = top3.iloc[1][\"Best Estimator\"]\n",
    "best_model_3 = top3.iloc[2][\"Best Estimator\"]\n",
    "\n",
    "base_models = []\n",
    "for i in range(len(top3)):\n",
    "    model_name = top3.loc[i, \"Model\"]\n",
    "    estimator = top3.loc[i, \"Best Estimator\"]\n",
    "    base_models.append((model_name, estimator))\n",
    "\n",
    "print(\"\\nTop 3 Models:\\n\", base_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "173166b1-80d7-4f36-b825-a1305b0ab81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best weights = (w1=0.72, w2=0.28, w3=0.00); OOF R^2=0.97897\n",
      "Saved submission_v12_3model_weighted.csv\n"
     ]
    }
   ],
   "source": [
    "def search_ensemble_weights_2(modelA, modelB, X, y, skf, increments=0.01):\n",
    "    \"\"\"\n",
    "    For 2-model ensemble:\n",
    "      preds = w1 * predsA + (1-w1) * predsB\n",
    "    We'll pick w1 in [0,1] in steps of increments.\n",
    "    Evaluate R2 in each fold, pick best average R2.\n",
    "    Returns best_w1, best_r2\n",
    "    \"\"\"\n",
    "    # We'll generate out-of-fold predictions for each model:\n",
    "    #  then combine them in each fold for each candidate weight.\n",
    "    predsA_oof = np.zeros(len(X))\n",
    "    predsB_oof = np.zeros(len(X))\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, y_bins):\n",
    "        XA, XV = X[train_idx], X[valid_idx]\n",
    "        ya, yv = y[train_idx], y[valid_idx]\n",
    "        mA = clone(modelA)\n",
    "        mB = clone(modelB)\n",
    "\n",
    "        mA.fit(XA, ya)\n",
    "        mB.fit(XA, ya)\n",
    "\n",
    "        predsA_oof[valid_idx] = mA.predict(XV)\n",
    "        predsB_oof[valid_idx] = mB.predict(XV)\n",
    "\n",
    "    # Now we have out-of-fold preds for A and B => evaluate all w1\n",
    "    best_w = 0\n",
    "    best_r2 = -999\n",
    "    w_candidates = np.arange(0, 1.0 + increments, increments)\n",
    "    for w1 in w_candidates:\n",
    "        ensemble_oof = w1 * predsA_oof + (1 - w1) * predsB_oof\n",
    "        r2_ens = r2_score(y, ensemble_oof)\n",
    "        if r2_ens > best_r2:\n",
    "            best_r2 = r2_ens\n",
    "            best_w = w1\n",
    "\n",
    "    return best_w, best_r2\n",
    "\n",
    "# Similarly for top 3:\n",
    "def search_ensemble_weights_3(modelA, modelB, modelC, X, y, skf, increments=0.01):\n",
    "    \"\"\"\n",
    "    For 3-model ensemble:\n",
    "      preds = w1*predA + w2*predB + w3*predC\n",
    "    subject to w1 + w2 + w3 = 1, w_i >= 0\n",
    "    We'll brute-force a small grid in increments.\n",
    "    \"\"\"\n",
    "    predsA_oof = np.zeros(len(X))\n",
    "    predsB_oof = np.zeros(len(X))\n",
    "    predsC_oof = np.zeros(len(X))\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, y_bins):\n",
    "        XA, XV = X[train_idx], X[valid_idx]\n",
    "        ya, yv = y[train_idx], y[valid_idx]\n",
    "        mA = clone(modelA)\n",
    "        mB = clone(modelB)\n",
    "        mC = clone(modelC)\n",
    "\n",
    "        mA.fit(XA, ya)\n",
    "        mB.fit(XA, ya)\n",
    "        mC.fit(XA, ya)\n",
    "\n",
    "        predsA_oof[valid_idx] = mA.predict(XV)\n",
    "        predsB_oof[valid_idx] = mB.predict(XV)\n",
    "        predsC_oof[valid_idx] = mC.predict(XV)\n",
    "\n",
    "    w_candidates = np.arange(0, 1.0 + increments, increments)\n",
    "    best_combo = (0,0,0)\n",
    "    best_r2 = -999\n",
    "    for w1 in w_candidates:\n",
    "        for w2 in w_candidates:\n",
    "            w3 = 1 - w1 - w2\n",
    "            if w3 < 0: \n",
    "                # skip invalid combos\n",
    "                continue\n",
    "            # Compute ensemble\n",
    "            ensemble_oof = w1*predsA_oof + w2*predsB_oof + w3*predsC_oof\n",
    "            r2_ens = r2_score(y, ensemble_oof)\n",
    "            if r2_ens > best_r2:\n",
    "                best_r2 = r2_ens\n",
    "                best_combo = (w1, w2, w3)\n",
    "\n",
    "    return best_combo, best_r2\n",
    "\n",
    "# Example usage after we find top 2 or 3 in results_df:\n",
    "modelA = results_df.iloc[0][\"Best Estimator\"]\n",
    "modelB = results_df.iloc[1][\"Best Estimator\"]\n",
    "modelC = results_df.iloc[2][\"Best Estimator\"]\n",
    "# best_w1, best_r2 = search_ensemble_weights_2(modelA, modelB, X, y, skf, increments=0.01)\n",
    "# # Then we refit modelA, modelB on the entire dataset -> final ensemble weights => predict X_val.\n",
    "\n",
    "# # -------------------------------------------------------------------------------------\n",
    "# # 4. Refit best models + apply best weights\n",
    "# # -------------------------------------------------------------------------------------\n",
    "# # If we found best w1 for top 2:\n",
    "# finalA = clone(modelA).fit(X, y)\n",
    "# finalB = clone(modelB).fit(X, y)\n",
    "# predsA_val = finalA.predict(X_val)\n",
    "# predsB_val = finalB.predict(X_val)\n",
    "# final_ensemble_val = best_w1*predsA_val + (1 - best_w1)*predsB_val\n",
    "\n",
    "# # Then create submission.\n",
    "# df_val[\"UHI Index\"] = final_ensemble_val\n",
    "\n",
    "# # Save the submission\n",
    "# os.makedirs(\"output\", exist_ok=True)\n",
    "# df_val.to_csv(\"output/submission_v12e.csv\", index=False)\n",
    "# print(\"Saved submission_12e.csv\")\n",
    "\n",
    "# 1) Find best 3-model weights with smaller increments\n",
    "best_combo, best_r2 = search_ensemble_weights_3(modelA, modelB, modelC, X, y, skf, increments=0.01)\n",
    "w1, w2, w3 = best_combo\n",
    "print(f\"Best weights = (w1={w1:.2f}, w2={w2:.2f}, w3={w3:.2f}); OOF R^2={best_r2:.5f}\")\n",
    "\n",
    "# 2) Refit all 3 base models on full data\n",
    "finalA = clone(modelA).fit(X, y)\n",
    "finalB = clone(modelB).fit(X, y)\n",
    "finalC = clone(modelC).fit(X, y)\n",
    "\n",
    "# 3) Predict on X_val from each model\n",
    "predA_val = finalA.predict(X_val)\n",
    "predB_val = finalB.predict(X_val)\n",
    "predC_val = finalC.predict(X_val)\n",
    "\n",
    "# 4) Combine using the found weights\n",
    "final_ensemble_val = w1*predA_val + w2*predB_val + w3*predC_val\n",
    "\n",
    "# 5) Create & save submission\n",
    "df_val[\"UHI Index\"] = final_ensemble_val\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "df_val.to_csv(\"output/submission_v12_3model_weighted.csv\", index=False)\n",
    "print(\"Saved submission_v12_3model_weighted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b7e1706-c8b0-443e-bb56-63520559f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_12.csv\n"
     ]
    }
   ],
   "source": [
    "# just a simple ensemble with top N models and average\n",
    "\n",
    "val_preds1 = best_model_1.predict(X_val)\n",
    "val_preds2 = best_model_2.predict(X_val)\n",
    "val_preds3 = best_model_3.predict(X_val)\n",
    "ensemble_preds = (val_preds1 + val_preds2 + val_preds3) / 3\n",
    "\n",
    "# Add predictions to the validation dataframe\n",
    "df_val[\"UHI Index\"] = ensemble_preds\n",
    "\n",
    "# Save the submission\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "df_val.to_csv(\"output/submission_v12.csv\", index=False)\n",
    "print(\"Saved submission_12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "855bd8f1-872b-4038-b30f-f0837d9b603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: KNeighbors; KNeighborsRegressor(algorithm='brute', n_neighbors=2, p=4, weights='distance')\n",
      "\t[    0     1     3 ... 11226 11227 11228], [    2     7     9 ... 11179 11191 11203]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   12    24    38 ... 11205 11210 11215]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    6     8    22 ... 11204 11209 11221]\n",
      "\t[    0     1     2 ... 11225 11226 11227], [   10    14    17 ... 11134 11213 11228]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   16    18    20 ... 11161 11173 11220]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   28    39    51 ... 11217 11219 11222]\n",
      "\t[    0     1     2 ... 11225 11226 11228], [    3    15    30 ... 11201 11208 11227]\n",
      "\t[    0     2     3 ... 11222 11227 11228], [    1     5    32 ... 11224 11225 11226]\n",
      "\t[    1     2     3 ... 11226 11227 11228], [    0    13    25 ... 11184 11199 11218]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    4    19    21 ... 11198 11202 11207]\n",
      "1: ExtraTrees; ExtraTreesRegressor(max_depth=50, n_estimators=1000, random_state=42)\n",
      "\t[    0     1     3 ... 11226 11227 11228], [    2     7     9 ... 11179 11191 11203]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   12    24    38 ... 11205 11210 11215]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    6     8    22 ... 11204 11209 11221]\n",
      "\t[    0     1     2 ... 11225 11226 11227], [   10    14    17 ... 11134 11213 11228]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   16    18    20 ... 11161 11173 11220]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   28    39    51 ... 11217 11219 11222]\n",
      "\t[    0     1     2 ... 11225 11226 11228], [    3    15    30 ... 11201 11208 11227]\n",
      "\t[    0     2     3 ... 11222 11227 11228], [    1     5    32 ... 11224 11225 11226]\n",
      "\t[    1     2     3 ... 11226 11227 11228], [    0    13    25 ... 11184 11199 11218]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    4    19    21 ... 11198 11202 11207]\n",
      "2: XGBoost; XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric='rmse', feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=25, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=2000, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...)\n",
      "\t[    0     1     3 ... 11226 11227 11228], [    2     7     9 ... 11179 11191 11203]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   12    24    38 ... 11205 11210 11215]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    6     8    22 ... 11204 11209 11221]\n",
      "\t[    0     1     2 ... 11225 11226 11227], [   10    14    17 ... 11134 11213 11228]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   16    18    20 ... 11161 11173 11220]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   28    39    51 ... 11217 11219 11222]\n",
      "\t[    0     1     2 ... 11225 11226 11228], [    3    15    30 ... 11201 11208 11227]\n",
      "\t[    0     2     3 ... 11222 11227 11228], [    1     5    32 ... 11224 11225 11226]\n",
      "\t[    1     2     3 ... 11226 11227 11228], [    0    13    25 ... 11184 11199 11218]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    4    19    21 ... 11198 11202 11207]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.02821517, 1.02549267, 1.02039576],\n",
       "       [1.02724674, 1.02445553, 1.02353883],\n",
       "       [1.02707417, 1.02672238, 1.02468228],\n",
       "       ...,\n",
       "       [0.98197398, 0.98066278, 0.98051852],\n",
       "       [0.98228557, 0.98282085, 0.9828307 ],\n",
       "       [0.98120508, 0.98117871, 0.98096257]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's do a custom out-of-fold prediction approach for these top 3\n",
    "\n",
    "# A) Generate OOF predictions for each base model\n",
    "#    We'll create arrays of shape [n_samples, n_base_models]\n",
    "oof_preds = np.zeros((len(X), len(base_models)))\n",
    "\n",
    "for idx, (mname, base_model) in enumerate(base_models):\n",
    "    print(f\"{idx}: {mname}; {base_model}\")\n",
    "    # We'll do a new copy of the model so we don't re-fit the original\n",
    "    # or we can clone it\n",
    "    model_clone = clone(base_model)\n",
    "\n",
    "    # out-of-fold predictions\n",
    "    fold_idx = 0\n",
    "    for train_idx, valid_idx in skf.split(X, y_bins):\n",
    "        print(f\"\\t{train_idx}, {valid_idx}\")\n",
    "        X_trainF, X_validF = X[train_idx], X[valid_idx]\n",
    "        y_trainF, y_validF = y[train_idx], y[valid_idx]\n",
    "\n",
    "        model_clone.fit(X_trainF, y_trainF)\n",
    "        preds_validF = model_clone.predict(X_validF)\n",
    "        oof_preds[valid_idx, idx] = preds_validF\n",
    "\n",
    "oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54ffc5e0-5e3a-45a1-9fba-7d8cfb431cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Meta-Learner Comparison ===\n",
      "XGB => OOF R2: 0.99405\n",
      "RF => OOF R2: 0.98691\n",
      "Linear => OOF R2: 0.97899\n",
      "Lasso => OOF R2: 0.97738\n",
      "MLP => OOF R2: 0.97456\n",
      "Ridge => OOF R2: 0.96587\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(oof_preds)\n",
    "\n",
    "meta_learners = {\n",
    "    \"Linear\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=1e-5, random_state=42),\n",
    "    \"XGB\": xgb.XGBRegressor(n_estimators=2000, learning_rate=0.01,\n",
    "                            max_depth=25, random_state=42, \n",
    "                            subsample=0.7, colsample_bytree=0.9,\n",
    "                            eval_metric=\"rmse\", use_label_encoder=False),\n",
    "    # \"LightGBM\": lgb.LGBMRegressor(n_estimators=2000, learning_rate=0.1, \n",
    "    #                               min_child_samples=1, num_leaves=31,\n",
    "    #                               max_depth=None, random_state=42),\n",
    "    \"RF\": RandomForestRegressor(n_estimators=400, max_depth=10,\n",
    "                                random_state=42),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(256,128), activation=\"relu\",\n",
    "                        solver=\"adam\", max_iter=2000, random_state=42)\n",
    "}\n",
    "\n",
    "# We'll store results in a dict\n",
    "meta_results = {}\n",
    "\n",
    "for mname, meta_model in meta_learners.items():\n",
    "    # Fit on the entire training set (oof_preds => y)\n",
    "    meta_model.fit(oof_preds, y)\n",
    "    \n",
    "    # Evaluate OOF RÂ² on the *same* data used for training\n",
    "    # (some risk of overfitting, but a quick comparison is fine)\n",
    "    preds_oof = meta_model.predict(oof_preds)\n",
    "    r2_val = r2_score(y, preds_oof)\n",
    "    \n",
    "    meta_results[mname] = r2_val\n",
    "\n",
    "# Print each meta-learnerâ€™s OOF RÂ²\n",
    "print(\"=== Meta-Learner Comparison ===\")\n",
    "for mname, score in sorted(meta_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{mname} => OOF R2: {score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a0ffd98c-f754-4443-878d-6d7c4c34c1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best meta-learner: XGB, OOF R2 => 0.99405\n",
      "0: KNeighbors, KNeighborsRegressor(algorithm='brute', n_neighbors=2, p=4, weights='distance')\n",
      "1: ExtraTrees, ExtraTreesRegressor(max_depth=50, n_estimators=1000, random_state=42)\n",
      "2: XGBoost, XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.9, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric='rmse', feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=25, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=2000, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...)\n",
      "Refitted best meta-learner (XGB) on entire dataset.\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "# Suppose you have:\n",
    "# base_models = [(name1, model1), (name2, model2), ...]\n",
    "# meta_learners = { \"LightGBM\": best_LGB, \"XGB\": best_XGB, ... } from your previous cell\n",
    "# meta_results = { \"LightGBM\": 0.99822, \"XGB\": 0.98399, ... } mapping each meta-learner to an OOF RÂ²\n",
    "# scaler = StandardScaler() # previously fit on oof_preds\n",
    "# X, y => full training data\n",
    "# oof_preds => shape (n_samples, len(base_models))\n",
    "\n",
    "##############################################\n",
    "# 1) Identify the best meta-learner from OOF\n",
    "##############################################\n",
    "best_meta_name = max(meta_results, key=meta_results.get)\n",
    "best_meta_model = meta_learners[best_meta_name]\n",
    "print(f\"Best meta-learner: {best_meta_name}, OOF R2 => {meta_results[best_meta_name]:.5f}\")\n",
    "\n",
    "##############################################\n",
    "# 2) Refit each base model on the FULL data\n",
    "##############################################\n",
    "base_models_fitted = []\n",
    "full_preds_stack = np.zeros((len(X), len(base_models)))  # same shape logic as OOF, but now for entire data\n",
    "\n",
    "for idx, (mname, base_model) in enumerate(base_models):\n",
    "    print(f\"{idx}: {mname}, {base_model}\")\n",
    "    # clone to avoid reusing partial state\n",
    "    fm = clone(base_model)\n",
    "    fm.fit(X, y)\n",
    "    base_models_fitted.append((mname, fm))\n",
    "    # store predictions\n",
    "    full_preds_stack[:, idx] = fm.predict(X)\n",
    "\n",
    "##############################################\n",
    "# 3) Scale the stacked predictions\n",
    "##############################################\n",
    "full_preds_stack_scaled = scaler.transform(full_preds_stack)\n",
    "\n",
    "##############################################\n",
    "# 4) Refit the chosen meta-learner on FULL stack\n",
    "##############################################\n",
    "final_meta_learner = clone(best_meta_model)\n",
    "final_meta_learner.fit(full_preds_stack_scaled, y)\n",
    "\n",
    "print(f\"Refitted best meta-learner ({best_meta_name}) on entire dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6382ead5-0714-47e1-8075-f0768acca691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96401143, 0.96337295, 0.96401817, ..., 1.0396885 , 1.0373894 ,\n",
       "       1.0355791 ], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------\n",
    "# MAKE VALIDATION PREDICTIONS via meta-ensemble\n",
    "# -------------------------------------------\n",
    "\n",
    "# 1) Stack predictions from each base model\n",
    "val_stack = np.zeros((len(X_val), len(base_models_fitted)))\n",
    "for idx, (mname, fm) in enumerate(base_models_fitted):\n",
    "    val_stack[:, idx] = fm.predict(X_val)\n",
    "\n",
    "# 2) Scale the stacked predictions, using the same scaler fit on OOF\n",
    "val_stack_scaled = scaler.transform(val_stack)\n",
    "\n",
    "# 3) Meta-learner final predictions\n",
    "final_val_preds = final_meta_learner.predict(val_stack_scaled)\n",
    "final_val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1617e948-7124-4fd3-820c-2de28307698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output/submission_v12_meta.csv\n"
     ]
    }
   ],
   "source": [
    "# SAVE SUBMISSION\n",
    "df_val = pd.read_csv(\"./data/Submission_template_UHI2025-v2.csv\")\n",
    "df_val[\"UHI Index\"] = final_val_preds\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "submission_path = \"output/submission_v12_meta.csv\"\n",
    "df_val.to_csv(submission_path, index=False)\n",
    "print(f\"Saved {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc796ce-066c-4f00-9fea-02d245399559",
   "metadata": {},
   "source": [
    "### Save Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38828f6e-43b5-4aeb-a974-b32d3d94d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved base model: models/base_KNeighbors_model_0_v12.pkl\n",
      "Saved base model: models/base_ExtraTrees_model_1_v12.pkl\n",
      "Saved base model: models/base_XGBoost_model_2_v12.pkl\n",
      "Saved final meta-learner (XGB) => models/final_meta_learner_XGB_20250131_2008.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save each base model\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "for i, (mname, fm) in enumerate(base_models_fitted):\n",
    "    output_path = f\"models/base_{mname}_model_{i}_v12.pkl\"\n",
    "    dump(fm, output_path)\n",
    "    print(f\"Saved base model: {output_path}\")\n",
    "\n",
    "# Save final meta-learner\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "meta_output_path = f\"models/final_meta_learner_{best_meta_name}_{timestamp}.pkl\"\n",
    "dump(final_meta_learner, meta_output_path)\n",
    "print(f\"Saved final meta-learner ({best_meta_name}) => {meta_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebfe01a-d997-48cf-8db8-f338c8af58f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
