{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1696d75-d304-46ea-bbbc-9108207d4b3f",
   "metadata": {},
   "source": [
    "### Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce26c37a-c3bc-4fb6-a473-e446d0c42833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Models\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, StackingRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa64bfea-9b4c-4e19-a17f-094c302b4bc0",
   "metadata": {},
   "source": [
    "### Feature Toggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fb07d95-c997-48ba-9bc1-6ed05fd3f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_FLAGS = {\n",
    "    \"building_coverage_50m\": True,\n",
    "    \"building_coverage_100m\": True,  # new\n",
    "    \"building_coverage_200m\": True,  # new\n",
    "    \"distance_water\": True,\n",
    "    \"distance_parks\": True,\n",
    "    \"lst_value\": True,\n",
    "    \"ndvi_value\": True,\n",
    "    \"ndbi_value\": True,\n",
    "    \"ndwi_value\": True,\n",
    "    \"evi_value\": True,              # new EVI\n",
    "    \"kmeans_location_cluster\": True,\n",
    "    \"dist_manhattan_centre\": True,\n",
    "    \"dist_bronx_centre\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2cdb4-d674-4380-89e7-47f59c90c474",
   "metadata": {},
   "source": [
    "### Load Borough Boundaries & Building Footprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e76b6bfd-7d16-4de4-af39-7ba6ae4768e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boroughs loaded: ['Staten Island' 'Queens' 'Brooklyn' 'Manhattan' 'Bronx']\n",
      "gdf buildings count before: Name           9436\n",
      "Description    9436\n",
      "geometry       9436\n",
      "dtype: int64\n",
      "Building columns after join: Index(['Name', 'Description', 'geometry', 'index_right', 'BoroName',\n",
      "       'cartodb_id', 'created_at', 'updated_at'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load NYC borough boundaries from geojson\n",
    "gdf_boroughs = gpd.read_file(\"./data/nyc_boroughs.geojson\").to_crs(\"EPSG:2263\")\n",
    "\n",
    "# Extract borough names from the \"name\" column\n",
    "if \"name\" in gdf_boroughs.columns:\n",
    "    gdf_boroughs = gdf_boroughs.rename(columns={\"name\": \"BoroName\"})\n",
    "else:\n",
    "    raise ValueError(\"Expected a 'name' column in borough shapefile for borough names.\")\n",
    "\n",
    "print(\"Boroughs loaded:\", gdf_boroughs[\"BoroName\"].unique())\n",
    "\n",
    "# building footprints\n",
    "gdf_buildings = gpd.read_file(\"./data/Building_Footprint.kml\").to_crs(\"EPSG:2263\")\n",
    "print(f\"gdf buildings count before: {gdf_buildings.count()}\")\n",
    "\n",
    "# Spatially join buildings with borough boundaries\n",
    "gdf_buildings = gpd.sjoin(gdf_buildings, gdf_boroughs, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "# Check resulting columns\n",
    "print(\"Building columns after join:\", gdf_buildings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c54a6b3a-5c12-4097-a23b-f8848ab2fc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some buildings are not assigned to any borough. They will be dropped.\n",
      "gdf buildings count after dropna: Name           9422\n",
      "Description    9422\n",
      "geometry       9422\n",
      "index_right    9422\n",
      "BoroName       9422\n",
      "cartodb_id     9422\n",
      "created_at     9422\n",
      "updated_at     9422\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Ensure there are no missing borough names\n",
    "if gdf_buildings[\"BoroName\"].isnull().any():\n",
    "    print(\"Some buildings are not assigned to any borough. They will be dropped.\")\n",
    "    gdf_buildings = gdf_buildings.dropna(subset=[\"BoroName\"])\n",
    "    print(f\"gdf buildings count after dropna: {gdf_buildings.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4fdf12-b4fa-4707-a664-58748a17b36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>geometry</th>\n",
       "      <th>index_right</th>\n",
       "      <th>BoroName</th>\n",
       "      <th>cartodb_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((1006651.793 248309.569, 100656...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((1005842.639 248829.838, 100585...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((1006243.634 249006.538, 100625...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((1006227.161 249476.52, 1006235...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((1008500.118 249763.236, 100846...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9431</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((997358.658 223175.61, 997394.0...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9432</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((998198.469 222047.008, 998288....</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9433</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((997517.49 219375.698, 997618.2...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9434</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((997465.413 215819.05, 997419.4...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9435</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>MULTIPOLYGON (((996496.699 217310.85, 996432.4...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2013-03-09 02:42:03.692000+00:00</td>\n",
       "      <td>2013-03-09 02:42:03.989000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9422 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name Description                                           geometry  \\\n",
       "0                      MULTIPOLYGON (((1006651.793 248309.569, 100656...   \n",
       "1                      MULTIPOLYGON (((1005842.639 248829.838, 100585...   \n",
       "2                      MULTIPOLYGON (((1006243.634 249006.538, 100625...   \n",
       "3                      MULTIPOLYGON (((1006227.161 249476.52, 1006235...   \n",
       "4                      MULTIPOLYGON (((1008500.118 249763.236, 100846...   \n",
       "...   ...         ...                                                ...   \n",
       "9431                   MULTIPOLYGON (((997358.658 223175.61, 997394.0...   \n",
       "9432                   MULTIPOLYGON (((998198.469 222047.008, 998288....   \n",
       "9433                   MULTIPOLYGON (((997517.49 219375.698, 997618.2...   \n",
       "9434                   MULTIPOLYGON (((997465.413 215819.05, 997419.4...   \n",
       "9435                   MULTIPOLYGON (((996496.699 217310.85, 996432.4...   \n",
       "\n",
       "      index_right   BoroName  cartodb_id                       created_at  \\\n",
       "0             4.0      Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
       "1             4.0      Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
       "2             4.0      Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
       "3             4.0      Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
       "4             4.0      Bronx         5.0 2013-03-09 02:42:03.692000+00:00   \n",
       "...           ...        ...         ...                              ...   \n",
       "9431          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
       "9432          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
       "9433          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
       "9434          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
       "9435          3.0  Manhattan         4.0 2013-03-09 02:42:03.692000+00:00   \n",
       "\n",
       "                           updated_at  \n",
       "0    2013-03-09 02:42:03.989000+00:00  \n",
       "1    2013-03-09 02:42:03.989000+00:00  \n",
       "2    2013-03-09 02:42:03.989000+00:00  \n",
       "3    2013-03-09 02:42:03.989000+00:00  \n",
       "4    2013-03-09 02:42:03.989000+00:00  \n",
       "...                               ...  \n",
       "9431 2013-03-09 02:42:03.989000+00:00  \n",
       "9432 2013-03-09 02:42:03.989000+00:00  \n",
       "9433 2013-03-09 02:42:03.989000+00:00  \n",
       "9434 2013-03-09 02:42:03.989000+00:00  \n",
       "9435 2013-03-09 02:42:03.989000+00:00  \n",
       "\n",
       "[9422 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_buildings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5914c1-d918-487b-b272-2a761f5c953c",
   "metadata": {},
   "source": [
    "### Subset Footprints for Manhattan & Bronx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ff450a7-2d58-437f-a278-71dbc8ec74c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan building footprints: 3495\n",
      "Bronx building footprints: 5927\n"
     ]
    }
   ],
   "source": [
    "# Subset by borough name\n",
    "gdf_manhattan = gdf_buildings[gdf_buildings[\"BoroName\"] == \"Manhattan\"].copy()\n",
    "gdf_bronx = gdf_buildings[gdf_buildings[\"BoroName\"] == \"Bronx\"].copy()\n",
    "\n",
    "print(f\"Manhattan building footprints: {len(gdf_manhattan)}\")\n",
    "print(f\"Bronx building footprints: {len(gdf_bronx)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c71abc6-523b-4e59-8cac-970033894ca0",
   "metadata": {},
   "source": [
    "### Find Largest Cluster Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64603ee8-e39b-426a-a063-35143e3180b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manhattan cluster center: 997790.3, 232244.6\n",
      "Bronx cluster center: 1017485.9, 241760.3\n"
     ]
    }
   ],
   "source": [
    "def find_largest_building_cluster_center(gdf_buildings_boro, eps_m=200, min_samples=10):\n",
    "    # 1. compute footprint centroids\n",
    "    building_centroids = gdf_buildings_boro.geometry.centroid\n",
    "    coords = np.column_stack([building_centroids.x, building_centroids.y])\n",
    "\n",
    "    # 2. cluster with DBSCAN\n",
    "    db = DBSCAN(eps=eps_m, min_samples=min_samples).fit(coords)\n",
    "    labels = db.labels_  # -1 => outliers\n",
    "\n",
    "    # 3. find largest cluster (excluding -1)\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    # ignore outliers\n",
    "    valid_mask = (unique >= 0)\n",
    "    if not np.any(valid_mask):\n",
    "        # no valid cluster => pick entire set's centroid?\n",
    "        mp = MultiPoint(coords)\n",
    "        return mp.centroid\n",
    "    # among the valid clusters, pick the largest\n",
    "    valid_clusters = unique[valid_mask]\n",
    "    valid_counts   = counts[valid_mask]\n",
    "    largest_id = valid_clusters[np.argmax(valid_counts)]\n",
    "    \n",
    "    # 4. gather coords in largest cluster\n",
    "    in_largest = coords[labels==largest_id]\n",
    "    if len(in_largest)==0:\n",
    "        # fallback\n",
    "        mp = MultiPoint(coords)\n",
    "        return mp.centroid\n",
    "    else:\n",
    "        return MultiPoint(in_largest).centroid\n",
    "\n",
    "manhattan_center_geom = find_largest_building_cluster_center(gdf_manhattan, eps_m=200, min_samples=20)\n",
    "bronx_center_geom = find_largest_building_cluster_center(gdf_bronx, eps_m=200, min_samples=20)\n",
    "\n",
    "manhattan_x, manhattan_y = manhattan_center_geom.x, manhattan_center_geom.y\n",
    "bronx_x, bronx_y = bronx_center_geom.x, bronx_center_geom.y\n",
    "\n",
    "print(f\"Manhattan cluster center: {manhattan_x:.1f}, {manhattan_y:.1f}\")\n",
    "print(f\"Bronx cluster center: {bronx_x:.1f}, {bronx_y:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9dcf5-87ba-44b2-9d6f-48422b31fd69",
   "metadata": {},
   "source": [
    "### Load Water, Parks, LST, S2 Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a929be55-da25-4ad2-bf7d-09971dfa4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "from shapely import wkt\n",
    "\n",
    "df_water = pd.read_csv(\"./data/NYC_Planimetric_Database__Hydrography_20250123.csv\")\n",
    "df_water[\"geometry\"] = df_water[\"the_geom\"].apply(wkt.loads)\n",
    "gdf_water = gpd.GeoDataFrame(df_water, geometry=\"geometry\", crs=\"EPSG:4326\").to_crs(\"EPSG:2263\")\n",
    "\n",
    "gdf_parks = gpd.read_file(\"./data/Parks_Properties_20250123.kml\").to_crs(\"EPSG:2263\")\n",
    "\n",
    "lst_raster = rxr.open_rasterio(\"Landsat_LST_v4_single_0601_0901.tiff\")\n",
    "lst_raster_2263 = lst_raster.rio.reproject(\"EPSG:2263\")\n",
    "\n",
    "indices_raster = rxr.open_rasterio(\"S2_indices_v4_single_0601_0901.tiff\")\n",
    "indices_raster_2263 = indices_raster.rio.reproject(\"EPSG:2263\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24501a2-6f17-4ea2-9c03-4be9918448e8",
   "metadata": {},
   "source": [
    "### Load Training Data & Reproject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7dd6df6-3cc7-4c38-9742-4c08339141b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (11229, 4)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./data/Training_data_uhi_index_UHI2025-v2.csv\")\n",
    "print(\"Train data shape:\", df_train.shape)\n",
    "\n",
    "gdf_train = gpd.GeoDataFrame(\n",
    "    df_train,\n",
    "    geometry=[Point(lon, lat) for lon, lat in zip(df_train.Longitude, df_train.Latitude)],\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(\"EPSG:2263\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931df14-1659-45c3-b180-079de0a151a9",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "182c7c38-4d48-456f-a3be-292f33d688ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose your \"indices_raster_2263\" is a 3-band stack: band=1 => NDVI, 2 => NDBI, 3 => NDWI\n",
    "# For EVI, we need actual reflectances from Blue, Red, NIR. If you have them in a separate 4-band or 5-band raster, great.\n",
    "# If not, you need to produce them. For demonstration, let's assume we have a sentinel raster with B02=blue, B04=red, B08=NIR, etc.\n",
    "# We'll show a function to compute EVI on the fly below.\n",
    "\n",
    "def compute_evi(blue, red, nir, L=1.0, C1=6.0, C2=7.5, G=2.5):\n",
    "    \"\"\"\n",
    "    Compute the Enhanced Vegetation Index (EVI).\n",
    "    EVI = G * (NIR - Red) / (NIR + C1*Red - C2*Blue + L)\n",
    "    Typically G=2.5, C1=6, C2=7.5, L=1\n",
    "    \"\"\"\n",
    "    evi = G * (nir - red) / (nir + C1 * red - C2 * blue + L)\n",
    "    return evi\n",
    "\n",
    "def building_coverage_fraction(geom, building_gdf, radius=50):\n",
    "    buffer_poly = geom.buffer(radius)\n",
    "    clipped = gpd.clip(building_gdf, buffer_poly)\n",
    "    area_buildings = clipped.geometry.area.sum()\n",
    "    area_buf = buffer_poly.area\n",
    "    return area_buildings / area_buf if area_buf > 0 else 0\n",
    "\n",
    "def distance_to_polygons(geom, poly_gdf):\n",
    "    dists = poly_gdf.geometry.distance(geom)\n",
    "    return dists.min() if len(dists) > 0 else np.nan\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    return np.sqrt((x1 - x2)**2 + (y1 - y2)**2)\n",
    "\n",
    "def extract_raster_value(geom, raster, band_index=1, method=\"nearest\"):\n",
    "    x, y = geom.x, geom.y\n",
    "    val = raster.sel(x=x, y=y, band=band_index, method=method).values\n",
    "    return float(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f045189e-eb48-4c08-bb5c-750cb59681a1",
   "metadata": {},
   "source": [
    "### Build Training Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811d8048-2b10-43b4-8ab0-fd7aacfe0234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD Features:\n",
    "bld_cov_50 = []\n",
    "bld_cov_100 = []\n",
    "bld_cov_200 = []\n",
    "dist_water_vals = []\n",
    "dist_parks_vals = []\n",
    "lst_vals = []\n",
    "ndvi_vals = []\n",
    "ndbi_vals = []\n",
    "ndwi_vals = []\n",
    "evi_vals = []\n",
    "dist_man_vals = []\n",
    "dist_bron_vals = []\n",
    "\n",
    "for i, row in gdf_train.iterrows():\n",
    "    geom = row.geometry\n",
    "\n",
    "    # coverage 50, 100, 200\n",
    "    if FEATURE_FLAGS[\"building_coverage_50m\"]:\n",
    "        bld_cov_50.append(building_coverage_fraction(geom, gdf_buildings, radius=50))\n",
    "    else:\n",
    "        bld_cov_50.append(0)\n",
    "\n",
    "    if FEATURE_FLAGS[\"building_coverage_100m\"]:\n",
    "        bld_cov_100.append(building_coverage_fraction(geom, gdf_buildings, radius=100))\n",
    "    else:\n",
    "        bld_cov_100.append(0)\n",
    "\n",
    "    if FEATURE_FLAGS[\"building_coverage_200m\"]:\n",
    "        bld_cov_200.append(building_coverage_fraction(geom, gdf_buildings, radius=200))\n",
    "    else:\n",
    "        bld_cov_200.append(0)\n",
    "\n",
    "    # distance water\n",
    "    dist_water_vals.append(distance_to_polygons(geom, gdf_water) if FEATURE_FLAGS[\"distance_water\"] else 0)\n",
    "    # distance parks\n",
    "    dist_parks_vals.append(distance_to_polygons(geom, gdf_parks) if FEATURE_FLAGS[\"distance_parks\"] else 0)\n",
    "    # LST\n",
    "    lst_val = extract_raster_value(geom, lst_raster_2263, band_index=1) if FEATURE_FLAGS[\"lst_value\"] else 0\n",
    "    lst_vals.append(lst_val)\n",
    "    # NDVI / NDBI / NDWI\n",
    "    ndv = extract_raster_value(geom, indices_raster_2263, 1) if FEATURE_FLAGS[\"ndvi_value\"] else 0\n",
    "    ndb = extract_raster_value(geom, indices_raster_2263, 2) if FEATURE_FLAGS[\"ndbi_value\"] else 0\n",
    "    ndw = extract_raster_value(geom, indices_raster_2263, 3) if FEATURE_FLAGS[\"ndwi_value\"] else 0\n",
    "    ndvi_vals.append(ndv)\n",
    "    ndbi_vals.append(ndb)\n",
    "    ndwi_vals.append(ndw)\n",
    "\n",
    "    # EVI (requires Blue/Red/NIR). If you only have NDVI, NDBI, NDWI in the raster, you might not have B02, B04, B08.\n",
    "    # If you do have them in some \"s2_bands_raster_2263\" with band indices: 1=Blue, 2=Green, 3=Red, 4=NIR, ...\n",
    "    # Then do something like:\n",
    "    if FEATURE_FLAGS[\"evi_value\"]:\n",
    "        # Example: let's pretend we have a 4-band sentinel raster with B02=1, B04=3, B08=4 for demonstration:\n",
    "        # For actual data, adapt indices accordingly.\n",
    "        # Or if you created a separate TIF with them, reference that:\n",
    "        try:\n",
    "            blue = extract_raster_value(geom, s2_bands_raster_2263, band_index=1)\n",
    "            red  = extract_raster_value(geom, s2_bands_raster_2263, band_index=3)\n",
    "            nir  = extract_raster_value(geom, s2_bands_raster_2263, band_index=4)\n",
    "            evi_val = compute_evi(blue, red, nir)\n",
    "        except:\n",
    "            evi_val = 0\n",
    "    else:\n",
    "        evi_val = 0\n",
    "    evi_vals.append(evi_val)\n",
    "\n",
    "    # Distance to Manhattan, Bronx centers\n",
    "    dist_man = euclidean_distance(geom.x, geom.y, manhattan_x, manhattan_y) if FEATURE_FLAGS[\"dist_manhattan_centre\"] else 0\n",
    "    dist_bro = euclidean_distance(geom.x, geom.y, bronx_x, bronx_y) if FEATURE_FLAGS[\"dist_bronx_centre\"] else 0\n",
    "    dist_man_vals.append(dist_man)\n",
    "    dist_bron_vals.append(dist_bro)\n",
    "\n",
    "gdf_train[\"bld_cov_50\"]   = bld_cov_50\n",
    "gdf_train[\"bld_cov_100\"]  = bld_cov_100\n",
    "gdf_train[\"bld_cov_200\"]  = bld_cov_200\n",
    "gdf_train[\"dist_water\"]   = dist_water_vals\n",
    "gdf_train[\"dist_parks\"]   = dist_parks_vals\n",
    "gdf_train[\"lst_value\"]    = lst_vals\n",
    "gdf_train[\"ndvi_value\"]   = ndvi_vals\n",
    "gdf_train[\"ndbi_value\"]   = ndbi_vals\n",
    "gdf_train[\"ndwi_value\"]   = ndwi_vals\n",
    "gdf_train[\"evi_value\"]    = evi_vals\n",
    "gdf_train[\"dist_manh\"]    = dist_man_vals\n",
    "gdf_train[\"dist_bron\"]    = dist_bron_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b16666a-1ae6-4eb1-89b5-3d9be782938b",
   "metadata": {},
   "source": [
    "### K-Means for location-based cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25f69df5-221a-4777-a561-fe74cf9f3aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLUSTERS = 20\n",
    "\n",
    "if FEATURE_FLAGS[\"kmeans_location_cluster\"]:\n",
    "    coords = np.column_stack([gdf_train.geometry.x, gdf_train.geometry.y])\n",
    "    kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=RANDOM_SEED, n_init=10).fit(coords)\n",
    "    gdf_train[\"location_cluster\"] = kmeans.labels_\n",
    "else:\n",
    "    kmeans = None\n",
    "    gdf_train[\"location_cluster\"] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa84f51-fa94-4968-a169-b7e196797da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>datetime</th>\n",
       "      <th>UHI Index</th>\n",
       "      <th>geometry</th>\n",
       "      <th>bld_cov_50</th>\n",
       "      <th>bld_cov_100</th>\n",
       "      <th>bld_cov_200</th>\n",
       "      <th>dist_water</th>\n",
       "      <th>dist_parks</th>\n",
       "      <th>lst_value</th>\n",
       "      <th>ndvi_value</th>\n",
       "      <th>ndbi_value</th>\n",
       "      <th>ndwi_value</th>\n",
       "      <th>evi_value</th>\n",
       "      <th>dist_manh</th>\n",
       "      <th>dist_bron</th>\n",
       "      <th>location_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-73.909167</td>\n",
       "      <td>40.813107</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.030289</td>\n",
       "      <td>POINT (1009393.606 235526.824)</td>\n",
       "      <td>0.019848</td>\n",
       "      <td>0.119473</td>\n",
       "      <td>0.248509</td>\n",
       "      <td>3536.717021</td>\n",
       "      <td>371.277131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12058.572295</td>\n",
       "      <td>10214.777740</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-73.909187</td>\n",
       "      <td>40.813045</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.030289</td>\n",
       "      <td>POINT (1009388.093 235504.35)</td>\n",
       "      <td>0.039795</td>\n",
       "      <td>0.159338</td>\n",
       "      <td>0.260397</td>\n",
       "      <td>3520.867715</td>\n",
       "      <td>358.889116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12047.167026</td>\n",
       "      <td>10232.870157</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-73.909215</td>\n",
       "      <td>40.812978</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.023798</td>\n",
       "      <td>POINT (1009380.276 235480.052)</td>\n",
       "      <td>0.026731</td>\n",
       "      <td>0.194538</td>\n",
       "      <td>0.278762</td>\n",
       "      <td>3504.846309</td>\n",
       "      <td>344.200944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12033.085822</td>\n",
       "      <td>10253.921329</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-73.909242</td>\n",
       "      <td>40.812908</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.023798</td>\n",
       "      <td>POINT (1009372.92 235454.541)</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.227561</td>\n",
       "      <td>0.312600</td>\n",
       "      <td>3487.673871</td>\n",
       "      <td>330.119247</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12019.162378</td>\n",
       "      <td>10275.373165</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-73.909257</td>\n",
       "      <td>40.812845</td>\n",
       "      <td>24-07-2021 15:53</td>\n",
       "      <td>1.021634</td>\n",
       "      <td>POINT (1009368.791 235431.463)</td>\n",
       "      <td>0.049321</td>\n",
       "      <td>0.263256</td>\n",
       "      <td>0.351619</td>\n",
       "      <td>3470.826136</td>\n",
       "      <td>320.290749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12009.039312</td>\n",
       "      <td>10292.806926</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11224</th>\n",
       "      <td>-73.957050</td>\n",
       "      <td>40.790333</td>\n",
       "      <td>24-07-2021 15:57</td>\n",
       "      <td>0.972470</td>\n",
       "      <td>POINT (996143.074 227219.577)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>569.780991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5288.111794</td>\n",
       "      <td>25825.361761</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11225</th>\n",
       "      <td>-73.957063</td>\n",
       "      <td>40.790308</td>\n",
       "      <td>24-07-2021 15:57</td>\n",
       "      <td>0.972470</td>\n",
       "      <td>POINT (996139.388 227210.467)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>559.973656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5297.917140</td>\n",
       "      <td>25833.538520</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11226</th>\n",
       "      <td>-73.957093</td>\n",
       "      <td>40.790270</td>\n",
       "      <td>24-07-2021 15:57</td>\n",
       "      <td>0.981124</td>\n",
       "      <td>POINT (996131.087 227196.498)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>543.787136</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5313.778307</td>\n",
       "      <td>25848.265634</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11227</th>\n",
       "      <td>-73.957112</td>\n",
       "      <td>40.790253</td>\n",
       "      <td>24-07-2021 15:59</td>\n",
       "      <td>0.981245</td>\n",
       "      <td>POINT (996126.012 227190.422)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>536.117477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5321.136077</td>\n",
       "      <td>25855.882277</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11228</th>\n",
       "      <td>-73.957128</td>\n",
       "      <td>40.790237</td>\n",
       "      <td>24-07-2021 15:59</td>\n",
       "      <td>0.983408</td>\n",
       "      <td>POINT (996121.402 227184.35)</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>528.656834</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5328.346048</td>\n",
       "      <td>25863.112636</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11229 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Longitude   Latitude          datetime  UHI Index  \\\n",
       "0     -73.909167  40.813107  24-07-2021 15:53   1.030289   \n",
       "1     -73.909187  40.813045  24-07-2021 15:53   1.030289   \n",
       "2     -73.909215  40.812978  24-07-2021 15:53   1.023798   \n",
       "3     -73.909242  40.812908  24-07-2021 15:53   1.023798   \n",
       "4     -73.909257  40.812845  24-07-2021 15:53   1.021634   \n",
       "...          ...        ...               ...        ...   \n",
       "11224 -73.957050  40.790333  24-07-2021 15:57   0.972470   \n",
       "11225 -73.957063  40.790308  24-07-2021 15:57   0.972470   \n",
       "11226 -73.957093  40.790270  24-07-2021 15:57   0.981124   \n",
       "11227 -73.957112  40.790253  24-07-2021 15:59   0.981245   \n",
       "11228 -73.957128  40.790237  24-07-2021 15:59   0.983408   \n",
       "\n",
       "                             geometry  bld_cov_50  bld_cov_100  bld_cov_200  \\\n",
       "0      POINT (1009393.606 235526.824)    0.019848     0.119473     0.248509   \n",
       "1       POINT (1009388.093 235504.35)    0.039795     0.159338     0.260397   \n",
       "2      POINT (1009380.276 235480.052)    0.026731     0.194538     0.278762   \n",
       "3       POINT (1009372.92 235454.541)    0.020690     0.227561     0.312600   \n",
       "4      POINT (1009368.791 235431.463)    0.049321     0.263256     0.351619   \n",
       "...                               ...         ...          ...          ...   \n",
       "11224   POINT (996143.074 227219.577)    0.000000     0.000000     0.000000   \n",
       "11225   POINT (996139.388 227210.467)    0.000000     0.000000     0.000000   \n",
       "11226   POINT (996131.087 227196.498)    0.000000     0.000000     0.000000   \n",
       "11227   POINT (996126.012 227190.422)    0.000000     0.000000     0.000000   \n",
       "11228    POINT (996121.402 227184.35)    0.000000     0.000000     0.000000   \n",
       "\n",
       "        dist_water  dist_parks  lst_value  ndvi_value  ndbi_value  ndwi_value  \\\n",
       "0      3536.717021  371.277131        NaN         NaN         NaN         NaN   \n",
       "1      3520.867715  358.889116        NaN         NaN         NaN         NaN   \n",
       "2      3504.846309  344.200944        NaN         NaN         NaN         NaN   \n",
       "3      3487.673871  330.119247        NaN         NaN         NaN         NaN   \n",
       "4      3470.826136  320.290749        NaN         NaN         NaN         NaN   \n",
       "...            ...         ...        ...         ...         ...         ...   \n",
       "11224   569.780991    0.000000        NaN         NaN         NaN         NaN   \n",
       "11225   559.973656    0.000000        NaN         NaN         NaN         NaN   \n",
       "11226   543.787136    0.000000        NaN         NaN         NaN         NaN   \n",
       "11227   536.117477    0.000000        NaN         NaN         NaN         NaN   \n",
       "11228   528.656834    0.000000        NaN         NaN         NaN         NaN   \n",
       "\n",
       "       evi_value     dist_manh     dist_bron  location_cluster  \n",
       "0              0  12058.572295  10214.777740                15  \n",
       "1              0  12047.167026  10232.870157                15  \n",
       "2              0  12033.085822  10253.921329                15  \n",
       "3              0  12019.162378  10275.373165                15  \n",
       "4              0  12009.039312  10292.806926                15  \n",
       "...          ...           ...           ...               ...  \n",
       "11224          0   5288.111794  25825.361761                 3  \n",
       "11225          0   5297.917140  25833.538520                 3  \n",
       "11226          0   5313.778307  25848.265634                 3  \n",
       "11227          0   5321.136077  25855.882277                 3  \n",
       "11228          0   5328.346048  25863.112636                 3  \n",
       "\n",
       "[11229 rows x 18 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf2817-a9ca-451f-94db-1e077e180047",
   "metadata": {},
   "source": [
    "### Final Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c73a2f73-5eaa-42a5-9c02-76d50dfecbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train feature shape: (11229, 13)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"bld_cov_50\", \"bld_cov_100\", \"bld_cov_200\",\n",
    "    \"dist_water\", \"dist_parks\",\n",
    "    \"lst_value\",\n",
    "    \"ndvi_value\", \"ndbi_value\", \"ndwi_value\", \"evi_value\",\n",
    "    \"dist_manh\", \"dist_bron\",\n",
    "    \"location_cluster\"\n",
    "]\n",
    "X = gdf_train[feature_cols].fillna(0.0).values\n",
    "y = gdf_train[\"UHI Index\"].values\n",
    "print(\"Final train feature shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f216e236-38bd-4d5f-a090-fc263396544d",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b327326e-7313-42f5-adbe-d0656c6ef1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stratified_bins(target, n_bins=10):\n",
    "    \"\"\"Bin the continuous target into discrete intervals for use in 'StratifiedKFold'.\"\"\"\n",
    "    # pd.qcut => quantile-based discretization\n",
    "    bins = pd.qcut(target, q=n_bins, duplicates=\"drop\")  # If duplicates occur, drop them\n",
    "    return bins.astype(str)  # Convert to string labels\n",
    "\n",
    "K_FOLDS = 5\n",
    "\n",
    "y_bins = make_stratified_bins(y, n_bins=10)\n",
    "skf = StratifiedKFold(n_splits=K_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "# kf = KFold(n_splits=K_FOLDS, shuffle=True, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75bda76a-4300-41b2-a859-b1f3acd94841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions and parameter grids\n",
    "models_and_params = {\n",
    "    \"RandomForest\": (\n",
    "        RandomForestRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"n_estimators\": [3000, 4000, 5000],\n",
    "            \"max_depth\": [None, 2, 4],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "            \"min_samples_split\": [4, 5, 6],\n",
    "        }\n",
    "    ),\n",
    "    \"ExtraTrees\": (\n",
    "        ExtraTreesRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"n_estimators\": [3000, 4000, 5000],\n",
    "            \"max_depth\": [None, 10, 20, 40, 50, 60],\n",
    "            \"min_samples_leaf\": [1, 2, 3],\n",
    "        }\n",
    "    ),\n",
    "    \"DecisionTree\": (\n",
    "        DecisionTreeRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"max_depth\": [60, 100, 120],\n",
    "            \"min_samples_leaf\": [1, 2],\n",
    "        }\n",
    "    ),\n",
    "    \"KNeighbors\": (\n",
    "        KNeighborsRegressor(),\n",
    "        {\n",
    "            \"n_neighbors\": [2, 3, 4, 5, 7, 10],\n",
    "            \"weights\": [\"uniform\", \"distance\"],\n",
    "            \"p\": [1, 2],  # Manhattan or Euclidean distance\n",
    "        }\n",
    "    ),\n",
    "    \"XGBoost\": (\n",
    "        XGBRegressor(random_state=RANDOM_SEED, eval_metric=\"rmse\", use_label_encoder=False),\n",
    "        {\n",
    "            \"n_estimators\": [1500, 2000, 2500, 3000],\n",
    "            \"max_depth\": [15, 20, 25],\n",
    "            \"learning_rate\": [0.005, 0.01, 0.02],\n",
    "            \"subsample\": [0.5, 0.6, 0.7],\n",
    "            \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "        }\n",
    "    ),\n",
    "    \"LightGBM\": (\n",
    "        lgb.LGBMRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"n_estimators\": [500, 1000],\n",
    "            \"max_depth\": [50, 80, 100],\n",
    "            \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "            \"subsample\": [0.6, 0.8, 1.0],\n",
    "            \"colsample_bytree\": [0.7, 0.8, 0.9],\n",
    "        }\n",
    "    ),\n",
    "    \"CatBoost\": (\n",
    "        CatBoostRegressor(silent=True, random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"iterations\": [500, 1000],\n",
    "            \"depth\": [10, 12, 14],\n",
    "            \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "            \"random_strength\": [1, 2, 3, 5],\n",
    "        }\n",
    "    ),\n",
    "    \"HistGradientBoosting\": (\n",
    "        HistGradientBoostingRegressor(random_state=RANDOM_SEED),\n",
    "        {\n",
    "            \"max_iter\": [500, 1000, 2000],\n",
    "            \"learning_rate\": [0.001, 0.01, 0.05, 0.1],\n",
    "            \"max_depth\": [None, 5, 10, 20],\n",
    "            \"max_leaf_nodes\": [15, 31, 63],\n",
    "            \"min_samples_leaf\": [10, 20, 50],\n",
    "            \"l2_regularization\": [0.0, 0.1, 1.0]\n",
    "        }\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c60c60b9-dcc8-4934-a4b7-7b3d0b392551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Searching RandomForest ===\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 17\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomizedSearchCV\n\u001b[0;32m      7\u001b[0m search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m      8\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m      9\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m best_estimator \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_estimator_\n\u001b[0;32m     19\u001b[0m best_score \u001b[38;5;241m=\u001b[39m search\u001b[38;5;241m.\u001b[39mbest_score_\n",
      "File \u001b[1;32mC:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1806\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1806\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1808\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1809\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mC:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\workspace\\python\\ai-challenge-2025\\.venv\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_ITER = 20\n",
    "results = []\n",
    "\n",
    "for model_name, (model, param_grid) in models_and_params.items():\n",
    "    print(f\"\\n=== Searching {model_name} ===\")\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=N_ITER,\n",
    "        cv=list(skf.split(X, y_bins)),\n",
    "        scoring=\"r2\",\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    search.fit(X, y)\n",
    "    best_estimator = search.best_estimator_\n",
    "    best_score = search.best_score_\n",
    "    best_params = search.best_params_\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Best Estimator\": best_estimator,\n",
    "        \"Best Score (CV)\": best_score,\n",
    "        \"Best Params\": best_params\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Best Score (CV)\", ascending=False)\n",
    "print(\"\\nFinal Cross-Val Results:\\n\", results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018bd501-3c56-42be-a688-c3546e79dea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('results_df.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a236c4f-6c3d-4bbe-b454-6001ed5ae71b",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11add13f-bff5-4cf2-8b1b-554334251d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv(\"./data/Submission_template_UHI2025-v2.csv\")\n",
    "gdf_val = gpd.GeoDataFrame(\n",
    "    df_val,\n",
    "    geometry=[Point(lon, lat) for lon, lat in zip(df_val.Longitude, df_val.Latitude)],\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(\"EPSG:2263\")\n",
    "\n",
    "cv_val = []\n",
    "dw_val = []\n",
    "dp_val = []\n",
    "lstv_val=[]\n",
    "ndv_val=[]\n",
    "ndbi_val=[]\n",
    "ndwi_val=[]\n",
    "distm_val=[]\n",
    "distb_val=[]\n",
    "\n",
    "for idx, row in gdf_val.iterrows():\n",
    "    geom = row.geometry\n",
    "    if FEATURE_FLAGS[\"building_coverage_50m\"]:\n",
    "        cfrac = building_coverage_fraction(geom,gdf_buildings,50)\n",
    "    else:\n",
    "        cfrac=0\n",
    "    cv_val.append(cfrac)\n",
    "\n",
    "    if FEATURE_FLAGS[\"distance_water\"]:\n",
    "        w=distance_to_polygons(geom,gdf_water)\n",
    "    else:\n",
    "        w=0\n",
    "    dw_val.append(w)\n",
    "\n",
    "    if FEATURE_FLAGS[\"distance_parks\"]:\n",
    "        p=distance_to_polygons(geom,gdf_parks)\n",
    "    else:\n",
    "        p=0\n",
    "    dp_val.append(p)\n",
    "\n",
    "    if FEATURE_FLAGS[\"lst_value\"]:\n",
    "        lv=extract_raster_value(geom, lst_raster_2263,1)\n",
    "    else:\n",
    "        lv=0\n",
    "    lstv_val.append(lv)\n",
    "\n",
    "    # NDVI, NDBI, NDWI\n",
    "    ndv=0\n",
    "    nb=0\n",
    "    nw=0\n",
    "    if FEATURE_FLAGS[\"ndvi_value\"]:\n",
    "        ndv=extract_raster_value(geom, indices_raster_2263,1)\n",
    "    if FEATURE_FLAGS[\"ndbi_value\"]:\n",
    "        nb=extract_raster_value(geom, indices_raster_2263,2)\n",
    "    if FEATURE_FLAGS[\"ndwi_value\"]:\n",
    "        nw=extract_raster_value(geom, indices_raster_2263,3)\n",
    "    ndv_val.append(ndv)\n",
    "    ndbi_val.append(nb)\n",
    "    ndwi_val.append(nw)\n",
    "\n",
    "    # dist manhattan\n",
    "    if FEATURE_FLAGS[\"dist_manhattan_centre\"]:\n",
    "        dm=euclidean_distance(geom.x, geom.y, manhattan_x, manhattan_y)\n",
    "    else:\n",
    "        dm=0\n",
    "    distm_val.append(dm)\n",
    "\n",
    "    # dist bronx\n",
    "    if FEATURE_FLAGS[\"dist_bronx_centre\"]:\n",
    "        db=euclidean_distance(geom.x, geom.y, bronx_x, bronx_y)\n",
    "    else:\n",
    "        db=0\n",
    "    distb_val.append(db)\n",
    "\n",
    "df_val[\"bld_cover_50m\"] = cv_val\n",
    "df_val[\"dist_water\"] = dw_val\n",
    "df_val[\"dist_parks\"] = dp_val\n",
    "df_val[\"lst_value\"] = lstv_val\n",
    "df_val[\"ndvi_value\"] = ndv_val\n",
    "df_val[\"ndbi_value\"] = ndbi_val\n",
    "df_val[\"ndwi_value\"] = ndwi_val\n",
    "df_val[\"dist_manhattan_centre\"] = distm_val  # Ensure column name matches\n",
    "df_val[\"dist_bronx_centre\"] = distb_val  # Ensure column name matches\n",
    "\n",
    "# Handle location clusters for validation\n",
    "if FEATURE_FLAGS[\"kmeans_location_cluster\"] and kmeans is not None:\n",
    "    coords_val = np.column_stack([gdf_val.geometry.x, gdf_val.geometry.y])\n",
    "    df_val[\"location_cluster\"] = kmeans.predict(coords_val)\n",
    "else:\n",
    "    df_val[\"location_cluster\"] = 0  # Default to cluster 0 if not using kmeans\n",
    "\n",
    "# Select features for validation\n",
    "df_val_feat = df_val[all_possible_features].fillna(0.0)\n",
    "X_val = df_val_feat.values\n",
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f3a93f-6c2b-45e7-9995-3eb039fe7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pick top 4 models from the results\n",
    "top4 = results_df.head(4).reset_index(drop=True)\n",
    "best_model_1 = top4.iloc[0][\"Best Estimator\"]\n",
    "best_model_2 = top4.iloc[1][\"Best Estimator\"]\n",
    "best_model_3 = top4.iloc[2][\"Best Estimator\"]\n",
    "\n",
    "base_models = []\n",
    "for i in range(len(top4)):\n",
    "    model_name = top4.loc[i, \"Model\"]\n",
    "    estimator = top4.loc[i, \"Best Estimator\"]\n",
    "    base_models.append((model_name, estimator))\n",
    "\n",
    "print(\"\\nTop 4 Models:\\n\", base_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173166b1-80d7-4f36-b825-a1305b0ab81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_ensemble_weights_2(modelA, modelB, X, y, skf, increments=0.05):\n",
    "    \"\"\"\n",
    "    For 2-model ensemble:\n",
    "      preds = w1 * predsA + (1-w1) * predsB\n",
    "    We'll pick w1 in [0,1] in steps of increments.\n",
    "    Evaluate R2 in each fold, pick best average R2.\n",
    "    Returns best_w1, best_r2\n",
    "    \"\"\"\n",
    "    # We'll generate out-of-fold predictions for each model:\n",
    "    #  then combine them in each fold for each candidate weight.\n",
    "    predsA_oof = np.zeros(len(X))\n",
    "    predsB_oof = np.zeros(len(X))\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, y_bins):\n",
    "        XA, XV = X[train_idx], X[valid_idx]\n",
    "        ya, yv = y[train_idx], y[valid_idx]\n",
    "        mA = clone(modelA)\n",
    "        mB = clone(modelB)\n",
    "\n",
    "        mA.fit(XA, ya)\n",
    "        mB.fit(XA, ya)\n",
    "\n",
    "        predsA_oof[valid_idx] = mA.predict(XV)\n",
    "        predsB_oof[valid_idx] = mB.predict(XV)\n",
    "\n",
    "    # Now we have out-of-fold preds for A and B => evaluate all w1\n",
    "    best_w = 0\n",
    "    best_r2 = -999\n",
    "    w_candidates = np.arange(0, 1.0 + increments, increments)\n",
    "    for w1 in w_candidates:\n",
    "        ensemble_oof = w1 * predsA_oof + (1 - w1) * predsB_oof\n",
    "        r2_ens = r2_score(y, ensemble_oof)\n",
    "        if r2_ens > best_r2:\n",
    "            best_r2 = r2_ens\n",
    "            best_w = w1\n",
    "\n",
    "    return best_w, best_r2\n",
    "\n",
    "# Similarly for top 3:\n",
    "def search_ensemble_weights_3(modelA, modelB, modelC, X, y, skf, increments=0.05):\n",
    "    \"\"\"\n",
    "    For 3-model ensemble:\n",
    "      preds = w1*predA + w2*predB + w3*predC\n",
    "    subject to w1 + w2 + w3 = 1, w_i >= 0\n",
    "    We'll brute-force a small grid in increments.\n",
    "    \"\"\"\n",
    "    predsA_oof = np.zeros(len(X))\n",
    "    predsB_oof = np.zeros(len(X))\n",
    "    predsC_oof = np.zeros(len(X))\n",
    "\n",
    "    for train_idx, valid_idx in skf.split(X, y_bins):\n",
    "        XA, XV = X[train_idx], X[valid_idx]\n",
    "        ya, yv = y[train_idx], y[valid_idx]\n",
    "        mA = clone(modelA)\n",
    "        mB = clone(modelB)\n",
    "        mC = clone(modelC)\n",
    "\n",
    "        mA.fit(XA, ya)\n",
    "        mB.fit(XA, ya)\n",
    "        mC.fit(XA, ya)\n",
    "\n",
    "        predsA_oof[valid_idx] = mA.predict(XV)\n",
    "        predsB_oof[valid_idx] = mB.predict(XV)\n",
    "        predsC_oof[valid_idx] = mC.predict(XV)\n",
    "\n",
    "    w_candidates = np.arange(0, 1.0 + increments, increments)\n",
    "    best_combo = (0,0,0)\n",
    "    best_r2 = -999\n",
    "    for w1 in w_candidates:\n",
    "        for w2 in w_candidates:\n",
    "            w3 = 1 - w1 - w2\n",
    "            if w3 < 0: \n",
    "                # skip invalid combos\n",
    "                continue\n",
    "            # Compute ensemble\n",
    "            ensemble_oof = w1*predsA_oof + w2*predsB_oof + w3*predsC_oof\n",
    "            r2_ens = r2_score(y, ensemble_oof)\n",
    "            if r2_ens > best_r2:\n",
    "                best_r2 = r2_ens\n",
    "                best_combo = (w1, w2, w3)\n",
    "\n",
    "    return best_combo, best_r2\n",
    "\n",
    "# Example usage after we find top 2 or 3 in results_df:\n",
    "# modelA = results_df.iloc[0][\"Best Estimator\"]\n",
    "# modelB = results_df.iloc[1][\"Best Estimator\"]\n",
    "# best_w1, best_r2 = search_ensemble_weights_2(modelA, modelB, X, y, skf)\n",
    "# Then we refit modelA, modelB on the entire dataset -> final ensemble weights => predict X_val.\n",
    "\n",
    "# -------------------------------------------------------------------------------------\n",
    "# 4. Refit best models + apply best weights\n",
    "# -------------------------------------------------------------------------------------\n",
    "# If we found best w1 for top 2:\n",
    "# finalA = clone(modelA).fit(X, y)\n",
    "# finalB = clone(modelB).fit(X, y)\n",
    "# predsA_val = finalA.predict(X_val)\n",
    "# predsB_val = finalB.predict(X_val)\n",
    "# final_ensemble_val = best_w1*predsA_val + (1 - best_w1)*predsB_val\n",
    "\n",
    "# Then create submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b7e1706-c8b0-443e-bb56-63520559f39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission_11.csv\n"
     ]
    }
   ],
   "source": [
    "val_preds1 = best_model_1.predict(X_val)\n",
    "val_preds2 = best_model_2.predict(X_val) if best_model_1 != best_model_2 else val_preds1\n",
    "val_preds3 = best_model_3.predict(X_val) if best_model_1 != best_model_3 else val_preds1\n",
    "ensemble_preds = (val_preds1 + val_preds2) / 2\n",
    "\n",
    "# Add predictions to the validation dataframe\n",
    "df_val[\"UHI Index\"] = ensemble_preds\n",
    "\n",
    "# Save the submission\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "df_val.to_csv(\"output/submission_v11.csv\", index=False)\n",
    "print(\"Saved submission_11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "855bd8f1-872b-4038-b30f-f0837d9b603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ExtraTrees; ExtraTreesRegressor(max_depth=50, n_estimators=3000, random_state=42)\n",
      "\t[    0     1     3 ... 11226 11227 11228], [    2     7     9 ... 11179 11191 11203]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   12    24    38 ... 11205 11210 11215]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    6     8    22 ... 11204 11209 11221]\n",
      "\t[    0     1     2 ... 11225 11226 11227], [   10    14    17 ... 11134 11213 11228]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   16    18    20 ... 11161 11173 11220]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   28    39    51 ... 11217 11219 11222]\n",
      "\t[    0     1     2 ... 11225 11226 11228], [    3    15    30 ... 11201 11208 11227]\n",
      "\t[    0     2     3 ... 11222 11227 11228], [    1     5    32 ... 11224 11225 11226]\n",
      "\t[    1     2     3 ... 11226 11227 11228], [    0    13    25 ... 11184 11199 11218]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    4    19    21 ... 11198 11202 11207]\n",
      "1: KNeighbors; KNeighborsRegressor(weights='distance')\n",
      "\t[    0     1     3 ... 11226 11227 11228], [    2     7     9 ... 11179 11191 11203]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   12    24    38 ... 11205 11210 11215]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    6     8    22 ... 11204 11209 11221]\n",
      "\t[    0     1     2 ... 11225 11226 11227], [   10    14    17 ... 11134 11213 11228]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   16    18    20 ... 11161 11173 11220]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   28    39    51 ... 11217 11219 11222]\n",
      "\t[    0     1     2 ... 11225 11226 11228], [    3    15    30 ... 11201 11208 11227]\n",
      "\t[    0     2     3 ... 11222 11227 11228], [    1     5    32 ... 11224 11225 11226]\n",
      "\t[    1     2     3 ... 11226 11227 11228], [    0    13    25 ... 11184 11199 11218]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    4    19    21 ... 11198 11202 11207]\n",
      "2: XGBoost; XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric='rmse', feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=20, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=2000, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...)\n",
      "\t[    0     1     3 ... 11226 11227 11228], [    2     7     9 ... 11179 11191 11203]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   12    24    38 ... 11205 11210 11215]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    6     8    22 ... 11204 11209 11221]\n",
      "\t[    0     1     2 ... 11225 11226 11227], [   10    14    17 ... 11134 11213 11228]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   16    18    20 ... 11161 11173 11220]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   28    39    51 ... 11217 11219 11222]\n",
      "\t[    0     1     2 ... 11225 11226 11228], [    3    15    30 ... 11201 11208 11227]\n",
      "\t[    0     2     3 ... 11222 11227 11228], [    1     5    32 ... 11224 11225 11226]\n",
      "\t[    1     2     3 ... 11226 11227 11228], [    0    13    25 ... 11184 11199 11218]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    4    19    21 ... 11198 11202 11207]\n",
      "3: RandomForest; RandomForestRegressor(min_samples_split=5, n_estimators=4000, random_state=42)\n",
      "\t[    0     1     3 ... 11226 11227 11228], [    2     7     9 ... 11179 11191 11203]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   12    24    38 ... 11205 11210 11215]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    6     8    22 ... 11204 11209 11221]\n",
      "\t[    0     1     2 ... 11225 11226 11227], [   10    14    17 ... 11134 11213 11228]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   16    18    20 ... 11161 11173 11220]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [   28    39    51 ... 11217 11219 11222]\n",
      "\t[    0     1     2 ... 11225 11226 11228], [    3    15    30 ... 11201 11208 11227]\n",
      "\t[    0     2     3 ... 11222 11227 11228], [    1     5    32 ... 11224 11225 11226]\n",
      "\t[    1     2     3 ... 11226 11227 11228], [    0    13    25 ... 11184 11199 11218]\n",
      "\t[    0     1     2 ... 11226 11227 11228], [    4    19    21 ... 11198 11202 11207]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 27\u001b[0m\n\u001b[0;32m     22\u001b[0m         oof_preds[valid_idx, idx] \u001b[38;5;241m=\u001b[39m preds_validF\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Evaluate how these base model OOF stacks do if we just pick the best column\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# But let's do a meta-learner approach:\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# B) Train a small meta-learner on oof_preds => y\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m meta_learner \u001b[38;5;241m=\u001b[39m \u001b[43mLinearRegression\u001b[49m()  \u001b[38;5;66;03m# could also try Ridge, XGBoost, etc.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m meta_learner\u001b[38;5;241m.\u001b[39mfit(oof_preds, y)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Evaluate the R2 on the same OOF predictions (some overfitting risk, but let's see)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's do a custom out-of-fold prediction approach for these top 4\n",
    "\n",
    "# A) Generate OOF predictions for each base model\n",
    "#    We'll create arrays of shape [n_samples, n_base_models]\n",
    "oof_preds = np.zeros((len(X), len(base_models)))\n",
    "\n",
    "for idx, (mname, base_model) in enumerate(base_models):\n",
    "    print(f\"{idx}: {mname}; {base_model}\")\n",
    "    # We'll do a new copy of the model so we don't re-fit the original\n",
    "    # or we can clone it\n",
    "    model_clone = clone(base_model)\n",
    "\n",
    "    # out-of-fold predictions\n",
    "    fold_idx = 0\n",
    "    for train_idx, valid_idx in skf.split(X, y_bins):\n",
    "        print(f\"\\t{train_idx}, {valid_idx}\")\n",
    "        X_trainF, X_validF = X[train_idx], X[valid_idx]\n",
    "        y_trainF, y_validF = y[train_idx], y[valid_idx]\n",
    "\n",
    "        model_clone.fit(X_trainF, y_trainF)\n",
    "        preds_validF = model_clone.predict(X_validF)\n",
    "        oof_preds[valid_idx, idx] = preds_validF\n",
    "\n",
    "oof_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54ffc5e0-5e3a-45a1-9fba-7d8cfb431cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-learner OOF R2: 0.97723\n"
     ]
    }
   ],
   "source": [
    "# Evaluate how these base model OOF stacks do if we just pick the best column\n",
    "# But let's do a meta-learner approach:\n",
    "# B) Train a small meta-learner on oof_preds => y\n",
    "meta_learner = LinearRegression()  # could also try Ridge, XGBoost, etc.\n",
    "meta_learner.fit(oof_preds, y)\n",
    "\n",
    "# Evaluate the R2 on the same OOF predictions (some overfitting risk, but let's see)\n",
    "meta_preds_oof = meta_learner.predict(oof_preds)\n",
    "r2_meta = r2_score(y, meta_preds_oof)\n",
    "print(f\"Meta-learner OOF R2: {r2_meta:.5f}\")\n",
    "# If that looks good, we proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef8b545f-95b9-4b07-b2a5-7c6f9e135851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ExtraTrees; ExtraTreesRegressor(max_depth=50, n_estimators=3000, random_state=42)\n",
      "1: KNeighbors; KNeighborsRegressor(weights='distance')\n",
      "2: XGBoost; XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric='rmse', feature_types=None,\n",
      "             gamma=None, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.01, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=20, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=2000, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...)\n",
      "3: RandomForest; RandomForestRegressor(min_samples_split=5, n_estimators=4000, random_state=42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Else we can also do a nested CV approach to measure meta-learner performance more robustly.\n",
    "\n",
    "# C) Refit each base model on the FULL training set, then produce meta features for the FULL set\n",
    "base_models_fitted = []\n",
    "full_preds_stack = np.zeros((len(X), len(base_models)))\n",
    "for idx, (mname, base_model) in enumerate(base_models):\n",
    "    print(f\"{idx}: {mname}; {base_model}\")\n",
    "    # clone to avoid reusing the partially-fitted model\n",
    "    fm = clone(base_model)\n",
    "    fm.fit(X, y)\n",
    "    base_models_fitted.append((mname, fm))\n",
    "    full_preds_stack[:, idx] = fm.predict(X)\n",
    "\n",
    "# Refit meta-learner on the full stacked predictions\n",
    "meta_learner_full = clone(meta_learner)\n",
    "meta_learner_full.fit(full_preds_stack, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6382ead5-0714-47e1-8075-f0768acca691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96319713, 0.96366751, 0.96178878, ..., 1.03934582, 1.0375195 ,\n",
       "       1.03582612])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 5. MAKE VALIDATION PREDICTIONS\n",
    "# We'll also create stacked features for X_val\n",
    "val_stack = np.zeros((len(X_val), len(base_models)))\n",
    "for idx, (mname, fm) in enumerate(base_models_fitted):\n",
    "    val_stack[:, idx] = fm.predict(X_val)\n",
    "\n",
    "val_preds_meta = meta_learner_full.predict(val_stack)\n",
    "# val_preds_meta = meta_learner.predict(val_stack)\n",
    "\n",
    "# Alternatively, you can do the simpler top2 average if that historically gave the best result:\n",
    "# e.g. top2 = base_models_fitted[:2]\n",
    "# val_preds_1 = top2[0][1].predict(X_val)\n",
    "# val_preds_2 = top2[1][1].predict(X_val)\n",
    "# val_preds_simple_avg = (val_preds_1 + val_preds_2) / 2\n",
    "\n",
    "# If you want to see if meta is better or top2 is better, you can do both and compare.\n",
    "# Let's define final predictions as the meta's:\n",
    "final_val_preds = val_preds_meta\n",
    "final_val_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1617e948-7124-4fd3-820c-2de28307698c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output/submission_v11-meta-preds.csv\n"
     ]
    }
   ],
   "source": [
    "# 6. SAVE SUBMISSION\n",
    "df_val = pd.read_csv(\"./data/Submission_template_UHI2025-v2.csv\")\n",
    "df_val[\"UHI Index\"] = final_val_preds\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "submission_path = \"output/submission_v11-meta-preds.csv\"\n",
    "df_val.to_csv(submission_path, index=False)\n",
    "print(f\"Saved {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "caaae49f-afa8-4103-ad67-ef120072fc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved output/submission_v11.csv\n"
     ]
    }
   ],
   "source": [
    "val_preds1 = best_model_1.predict(X_val)\n",
    "val_preds2 = best_model_2.predict(X_val) if best_model_1 != best_model_2 else val_preds1\n",
    "ensemble_preds = (val_preds1 + val_preds2) / 2\n",
    "\n",
    "# Add predictions to the validation dataframe\n",
    "df_val[\"UHI Index\"] = ensemble_preds\n",
    "\n",
    "# Save the submission\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "submission_path = \"output/submission_v11.csv\"\n",
    "df_val.to_csv(submission_path, index=False)\n",
    "print(f\"Saved {submission_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc796ce-066c-4f00-9fea-02d245399559",
   "metadata": {},
   "source": [
    "### Save Top Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38828f6e-43b5-4aeb-a974-b32d3d94d5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved base model: models/base_ExtraTrees_model_0_v11.pkl\n",
      "Saved base model: models/base_KNeighbors_model_1_v11.pkl\n",
      "Saved base model: models/base_XGBoost_model_2_v11.pkl\n",
      "Saved base model: models/base_RandomForest_model_3_v11.pkl\n",
      "Saved meta-learner.\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save base models\n",
    "for i, (mname, fm) in enumerate(base_models_fitted):\n",
    "    outpath = f\"models/base_{mname}_model_{i}_v11.pkl\"\n",
    "    dump(fm, outpath)\n",
    "    print(f\"Saved base model: {outpath}\")\n",
    "\n",
    "# Save meta-learner\n",
    "dump(meta_learner_full, \"models/meta_learner_linear_v11.pkl\")\n",
    "print(\"Saved meta-learner.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebfe01a-d997-48cf-8db8-f338c8af58f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
